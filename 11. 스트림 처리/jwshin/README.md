# 11장. 스트림 처리

## 1. 개요

- **배치**는 유한 입력을 모두 읽고 끝나는 처리(스냅샷 생성). 지연↑, 처리량 최적화.
- **스트림**은 무한 입력을 이벤트 발생 즉시 처리. 지연↓, 지속 실행.
- “하루 뒤 반영”이 느린 문제를 해결하려면 초/분 단위로 자주 처리하거나, 아예 **이벤트 단위 처리**로 전환.

| 항목 | 배치 처리 | 스트림 처리 |
| --- | --- | --- |
| 입력 | 유한(파일/스냅샷) | 무한(이벤트 흐름) |
| 실행 종료 | 있음 (입력 소진 시) | 없음 (지속 실행) |
| 결과 반영 | 완료 후 일괄 | 도착 즉시·지속적 |
| 목표 | 처리량 최적화 | 지연 최소화 |
| 사용 예 | 일일 리포트, 색인 빌드 | 실시간 알림, 모니터링, 실시간 랭킹 |

## 2. 이벤트 스트림 전송

- **왜 스트림 처리를 하는가?** 배치는 “끝이 있는 입력”을 다 읽고 결과를 만든다. 현실 데이터는 계속 들어오므로 하루/시간 단위 배치는 **지연이 크고** 폴링 오버헤드가 생긴다. 그래서 **이벤트가 생길 때마다 즉시 처리**하는 스트림을 사용한다.
- **이벤트란?** 타임스탬프를 가진 **작고 불변한 사실 1건**. JSON/Avro 등으로 직렬화되어 저장·전송된다.
- **전송 모델**: Producer → **Topic(파티션)** → Broker → Consumer Group
    - **그룹 내부**: 파티션을 나눠 **로드밸런싱**(각 파티션은 그룹 내 정확히 1개 소비자에게만).
    - **그룹 간**: **팬아웃**(여러 그룹이 같은 토픽을 각각 독립적으로 읽음).
    - **순서 보장**: **파티션 내부만** 보장, 파티션 간은 비보장.
    - **오프셋**으로 재처리/복구 가능(로그 보존이 전제).
- **배치 vs 스트림의 핵심 차이**: 배치는 파일을 써두고 나중에 읽음, 스트림은 **써지는 즉시 흘러감**. 지연·확장성·재처리 전략이 달라진다.

### 2-1. 메시징 시스템

- **왜 메시징 시스템인가**: 생산자–소비자를 1:1 소켓(유닉스 파이프나 TCP 연결)으로 묶으면 확장이 어렵다. 토픽 기반 발행/구독으로 다수-대-다수 연결, 독립 배포, 느슨한 결합을 얻는다.
- **속도 불일치 문제**: 생산 속도가 소비 속도를 앞지르면 지연·유실 중 하나를 택해야 한다.
    - 메시지를 버린다.
    - 큐에 메시지를 버퍼링한다. (큐 크기가 메모리 크기보다 더 커진다면??)
    - 생산자가 메시지를 더 보내지 못하게 막는다. (배압 backpressure, 흐름 제어 flow control)
- **일시적 장애 대응**: 메시지를 **영속 저장+복제**하고, 소비자 **오프셋**으로 재개하게 만들어 **리플레이**를 가능케 한다.
- **정확성 요구의 차등 적용**: 텔레메트리처럼 대략성 허용 도메인 vs 카운팅/정산처럼 **정확성 필수** 도메인 구분하여 전달 보장 수준을 결정.

### 2-2. 생산자에서 소비자로 메시지를 직접 전달하기

- 브로커 없는 직접 통신은 **속도와 단순성**을 준다(UDP 멀티캐스트, ZeroMQ, StatsD, Webhook).
- But, 오프라인·패킷손실·중복·순서 뒤틀림에 대한 **보장 부재**. 브로커가 해주던 **버퍼링/내구성/리플레이**가 없다는 점
- **언제 쓰나?** 지연 감소가 최우선이고, **일부 손실/중복을 감내**하거나 애플리케이션에서 보완 가능한 경우(시장 시세, 지표, 단순 알림).
- 직접 전송은 빠르지만 **운영/신뢰 책임이 애플리케이션으로 이동**한다. 데이터 유실이 곧 금전·정합성 문제인 도메인은 **브로커 기반**으로, 그 외는 직접 전송+보강으로 선택.

### 2-3. 메시지 브로커

- 메시지 브로커란?  생산자(Producer)가 보낸 메시지를 **중앙 서버(브로커)** 가 받아 저장해 두었다가 소비자(Consumer)가 가져가도록 중개하는 시스템.
- 왜 쓰나? **직접 전송(HTTP/UDP/Webhook)** 은 단순·저지연이 장점이지만, 상대 서비스 다운/느림/스파이크상황에서 다음과 같은 상황 대응이 어렵다.
    - 재시도·중복·유실 처리
    - 백프레셔
    - 버퍼링
- **메시지 브로커**는 중앙에서 **버퍼링·재시도·보존·모니터링**을 맡아 **생산자와 소비자를 느슨하게 결합**해준다.
- **저장과 내구성(속도↔안전 트레이드오프)**
    - **메모리 큐**: 아주 빠름, 장애 시 유실 위험.
    - **디스크 로그(append-only)**: 느리지만 내구성. 카프카는 **순차 쓰기 + 페이지 캐시 + 배치**로 고성능을 확보.
    - **복제**(replication)와 **fsync/배치 크기** 조합으로 내구성과 지연을 조정.
- **순서 보장과 병렬성**
    - 순서는 **큐/파티션 단위**로만 보장.
    - **소비자 그룹**은 한 파티션을 동시에 **1개 소비자만** 읽게 배정 → 그룹을 늘리면 처리량↑, 그러나 **파티션 수가 상한**.
    - **순서가 중요한 키**는 **같은 파티션으로 해시 파티셔닝**.

### 2-4. 메시지 브로커와 데이터베이스의 비교

- 메시지 브로커: 이벤트의 **흐름·전파·완충(버퍼링)·순서 보장**에 최적화.
- 데이터베이스: 데이터의 **영구 보관·임의 질의·집계**에 최적화.
- **보관 특성**
    - 브로커는 소비 확인(ack) 후 **메시지 삭제**가 기본. 장기 보관 목적이 아님.
    - 로그형 브로커(예: Kafka)는 **보존 정책/로그 컴팩션**으로 오래 둘 수 있지만, 질의 유연성은 제한적.
    - DB는 **명시 삭제 전까지 보존**하는 영구 스토리지.
- **읽기/조회 모델**
    - 브로커: 토픽/파티션을 **순차 소비**, 파티션 내 순서 보장. 임의 조건 조회는 약함.
    - DB: 인덱스·조인으로 **선택적/임의 조회** 및 복잡 집계에 강함.
- **처리량·확장**
    - 브로커: **큐 길이=지연**. 파티션 확장 + 소비자 수평 확장으로 처리량 맞춤.
    - DB: 스토리지·인덱스·쿼리 최적화로 대용량 **장기 데이터 처리**.
- **정합성·복구**
    - 브로커: 파티션 단위 **시간 순서**와 최소 한 번 전달이 일반적 → **멱등 키/아웃박스**로 중복 제거 설계.
    - DB: **스냅숏 일관성**으로 “현재 상태” 재현과 장애 복구에 유리.

### 2-5. 복수 소비자

<img width="721" height="316" alt="image" src="https://github.com/user-attachments/assets/6641fca7-68f6-4f0e-ae0f-64da50055441" />

- **로드 밸런싱 (shared subscription)**
    - **한 메시지 → 소비자 그룹 내 ‘하나’** 에게만 전달.
    - 목적: **처리량 확장**·비용 절감. 동일 작업을 여러 인스턴스가 **분담**.
    - 특징: 그룹 안에서 **경쟁 소비(competitive consumption)**, 개별 소비자는 서로 다른 메시지를 받음.
    - 순서: **파티션(또는 큐)** 단위로는 순서 유지되지만, **소비자 간 전체 순서 보장은 없음**.
    - 사용처: 이미지 변환, 로그 집계, 알림 발송 등 **동일 작업을 병렬화**할 때.
- **팬 아웃 (broadcast)**
    - 한 **메시지 → 모든 소비자**에게 **각각 전달**(독립 구독).
    - 목적: **여러 다운스트림**이 **서로 간섭 없이** 같은 이벤트를 활용.
    - 특징: 각 소비자는 **자신만의 오프셋/큐**로 읽음 → 서로의 처리 속도에 영향 없음.
    - 비용/주의: 소비자 수만큼 **네트워크·저장 비용 증가**, **중복 처리 대비(멱등)** 필요.
    - 사용처: **검색 색인 갱신, 추천 피드 업데이트, 모니터링/감사** 등 서로 다른 파이프라인에 동일 이벤트 배포.
- **두 패턴의 결합**
    - **여러 소비자 그룹**이 하나의 토픽을 구독하면 **그룹 간에는 팬 아웃**, **그룹 내부는 로드 밸런싱**으로 동작.
    - 예: “실시간 알림” 그룹(N개 인스턴스가 로드 밸런싱)과 “데이터 웨어하우스 적재” 그룹( M개 인스턴스)이 **동시에 같은 토픽**을 사용.
- **설계 체크포인트**
    - **스케일링**: 최대 동시 소비자 수 ≤ **파티션 수**(순서 보장/균등 분배 위해).
    - **순서 요구**: 순서가 중요하면 **키 기반 파티셔닝**으로 관련 이벤트를 같은 파티션에 모으기.
    - **신뢰성**: 중복·재시도 대비해 **멱등 처리/중복 제거 키** 설계.
    - **비용/부하**: 팬 아웃은 **소비자 수만큼 출력 부하**가 커짐—필요한 소비자만 붙이기.

### 2-6. 확인 응답과 재전송

- **왜 ACK(확인 응답)가 필요한가?** 소비자는 언제든 장애가 난다. 브로커는 메시지를 보낸 뒤 **소비자의 처리 완료 ACK(확인 응답)** 를 못 받으면 “처리 안 됐다”고 보고 **같은 메시지를 다른 소비자에게 재전송**한다. 네트워크에서 ACK가 유실돼도 같은 일이 벌어질 수 있음 → 기본 보장은 **at-least-once(최소 한 번 전달)**.
- **재전송은 메시지 순서를 깨지게 할 수 있다.**
    - 로드 밸런싱으로 여러 소비자가 병렬 처리 중일 때 한 소비자가 **중간 메시지에서 멈추면** 그 메시지가 **다른 소비자에게 뒤늦게** 넘어가며 **입력 순서가 뒤섞인다.**
    - 예시처럼 m1,m2… 순서로 생산돼도, 장애와 재전송 때문에 한 소비자는 **m4→m3→m5**처럼 받게 될 수 있다. 즉, **부하 분산 + 재전송을 함께 쓰면 전체 순서 보장은 사실상 불가**.

    <img width="593" height="316" alt="image" src="https://github.com/user-attachments/assets/f8595757-31db-4678-bb37-4b2303d9a05d" />

- 극복 방법
    - **순서가 중요한 키를 파티션 기준으로 묶고, 파티션당 단일 소비자**만 처리하게 하라(카프카 모델).
    - 재전송/중복은 전제하고 **멱등 처리**(메시지 ID로 중복 제거, 업서트, 누적 합산 등)로 방어.
    - 필요 시 메시지에 **오프셋/시퀀스**를 넣고 소비자 측 **재정렬 버퍼**를 쓰되 지연·메모리 비용을 감안.
    - 정말 **정확히 한 번**이 필요하면 트랜잭션(분산 커밋 등)을 고려하되, **비용과 복잡도**가 큼—가능하면 **at-least-once + 멱등**이 실무 기본.

### 2-7. 파티셔닝된 로그

- **전통 메시징 처리(JMS/AMQP)**: 브로커가 디스크에 쓸 수는 있지만 **소비자가 ACK 하면 바로 삭제**하는 **일시 보관 모델**. 새 소비자를 붙여도 **과거 메시지 재전송/재처리 불가**.
- **DB/파일 시스템**: 기본이 **영구 보관**. 입력을 **불변으로 보존**하니, 10장에서 본 일괄 처리처럼 **같은 입력을 여러 번 반복 실행**해 항상 동일 결과를 얻을 수 있음(재현성·복구 용이).
- 원하는 것은 **저장소의 영속성 + 메시징의 낮은 지연**을 동시에 갖는 것. 이를 해결하는 아이디어가 **로그 기반 메시지 브로커**.
- **로그 기반 브로커의 핵심**: 모든 이벤트를 **append-only 로그**에 **순서대로 기록**하고, **삭제를 지연**(보존 기간/컴팩션 정책)한다. 각 소비자는 **자신의 오프셋**으로 어디까지 읽었는지를 스스로 관리 → **과거 재읽기·재처리·리플레이** 가능.

### 2-8. 로그를 사용한 메시지 저장소

- **로그 기반 메시지 브로커의 기본 저장 모델**
    
    로그 기반 브로커는 메시지를 메모리 큐에만 두지 않고 **디스크의 로그**로 저장한다. 그래서 소비자가 늦게 붙어도 과거부터 다시 읽을 수 있고(재처리/리플레이), 단순 큐처럼 즉시 삭제되지 않는다.
    
- **토픽과 파티션**
    
    확장을 위해 하나의 토픽을 **여러 파티션**(독립 로그 파일)으로 쪼갠다. 파티션은 서로 다른 서버에 분산 배치 가능해서 **처리량과 저장량을 수평 확장**한다.

<img width="722" height="362" alt="image" src="https://github.com/user-attachments/assets/c0e1082f-8021-449d-a013-075141c95518" />
    
- **오프셋(Offset) 의미**
    
    파티션 내 각 메시지에 단조 증가하는 번호(오프셋)를 붙인다. **정렬/순서 보장은 파티션 내부까지만** 성립하고, **파티션 간 전역 순서는 보장하지 않는다.** 소비자는 “내가 어디까지 읽었는지”를 오프셋으로 관리한다.
    
- **생산자/소비자 흐름**
    
    생산자는 지정 규칙(키 해시 등)으로 특정 파티션에 메시지를 **Append**. 소비자는 자신이 배정받은 파티션을 **순차 읽기**하며, 각자 오프셋을 커밋해 진행 위치를 기록한다(중단 후 재개, 과거 재처리 가능).
    
- **새 소비자 추가·지연 합류에 유리**
    
    전통 메시징(JMS/AMQP)은 Ack 후 삭제되어 과거 복원이 어렵지만, 로그 기반은 **보존 기간(retention)** 동안 디스크에 남아 있어, 새로운 소비자도 **처음부터 재생**하거나 원하는 지점으로 **시점 복원**이 가능하다.
    
- **대용량·고처리량에 강함**
    
    Append-only 순차 쓰기/읽기라 디스크 효율이 높고, 파티션 병렬화로 **초당 수십만 건 수준**까지 스케일. 변경분 복제(Replication)로 **장애 대비**도 가능하다.
    

### 2-9. 로그 방식과 전통적인 메시징 방식의 비교

- **로그 기반 메시징(예: Kafka·Kinesis)**
    - 토픽을 **추가 전용 로그**로 보관한다. 소비자가 읽어도 메시지가 **삭제되지 않으므로** 여러 소비자가 **독립적으로 팬아웃**해 같은 메시지를 읽고, 필요하면 **재처리·리플레이**가 쉽다.
    - **소비자 그룹** 단위로 **파티션을 통째로 할당**받아 읽는다. 그룹 내부에서는 각 파티션을 **하나의 소비자만** 순서대로 처리하므로 **파티션 내 순서 보장**이 명확하다.
    - 브로커는 그룹의 노드 수에 맞춰 **로드 밸런싱(파티션 재할당)** 을 수행해 전체 파티션을 분배한다.
- **전통 메시징(JMS/AMQP 큐 모델)**
    - 기본은 **로드 밸런싱 큐**다. 메시지는 **한 소비자에게 전달되면 브로커에서 삭제**된다(리플레이 어려움).
    - **메시지 단위 병렬 처리**가 쉽다(메시지마다 다른 소비자로 분산). **순서 중요도가 낮은** 작업 분배에 적합하다.
    - 재전송·부하균형이 개입되면 **메시지 순서가 바뀔 수 있음**을 전제로 한다.

### 2-10. 소비자 오프셋

- **소비자 오프셋(consumer offset)** 은 각 파티션에서 “지금까지 읽은 마지막 위치”를 가리키는 포인터다. 오프셋보다 작은 번호의 메시지는 **이미 처리됨**, 더 큰 번호는 **아직 미처리**로 간주된다.
- 브로커는 개별 메시지에 대한 **확인 응답(ACK)을 추적하지 않는다.** 대신 소비자가 주기적으로 자기 오프셋을 기록(커밋)한다. 이 방식은 브로커 상태를 단순화하고 처리량을 높인다(파일/로그 기반 파이프라이닝에 유리).
- **장애/리밸런싱 시 동작**: 어떤 소비자 인스턴스가 죽으면 같은 그룹의 다른 인스턴스가 그 파티션을 **인계받아, 마지막으로 커밋된 오프셋부터** 처리를 재개한다.
- **중복 처리 가능성**: 메시지는 처리됐지만 오프셋 커밋이 그 전에 끝나지 못했다면, 재시작 후 그 메시지가 **한 번 더 처리**될 수 있다(기본은 *at-least-once*). 따라서 **멱등 처리**(idempotency)나 트랜잭션/EOS 기법으로 중복 영향 최소화가 필요하다.
- **순서 보장 범위**: 오프셋은 **파티션 단위 순서**를 보장한다. 파티션을 나누면 전체 토픽 전역 순서는 보장하지 않는다(확장성 ↔ 전역 순서의 트레이드오프).

### 2-11. 디스크 공간 사용

- 로그는 기본적으로 디스크 위 ‘원형 버퍼(링 버퍼)’다. 계속 append하면 언젠가 가득 차고, 공간을 재사용하려고 **세그먼트(조각)** 로 나눠 **가장 오래된 세그먼트부터 삭제/이동(보관)** 한다.
- **소비자가 느리면 유실 위험**: 소비자가 생산 속도를 못 따라가면, 소비자가 아직 읽지 못한 옛 세그먼트가 **보존 기간(retention)** 때문에 삭제될 수 있다. ⇒ 보존 기간을 **최소한 최대 lag보다 길게** 잡아야 한다.

### 2-12. 소비자가 생산자를 따라갈 수 없을 때

- 소비자가 처리 속도로 **생산자를 따라가지 못할 때** 선택지는 (1) 버리기, (2) 버퍼링, (3) 배압(backpressure)이다. **로그 기반 브로커(카프카류)** 는 기본적으로 **버퍼링**을 택한다.
- **로그 기반 브로커의 동작**
    - 브로커는 디스크에 **고정 크기(보존 기간/용량)** 의 로그 버퍼를 유지한다(링 버퍼).
    - 소비자 오프셋이 너무 뒤쳐져 **버퍼 범위를 벗어나면**, 가장 오래된 세그먼트부터 **자연스럽게 폐기**된다 → 그 **느린 소비자만** 해당 구간을 잃고, 다른 소비자는 **영향 없음**. (격리)
    - **헤드(최신 오프셋)와 소비자 오프셋 간 거리(lag)** 를 모니터링·알림하여 데이터 유실 전에 대처해야 한다.

### 2-13. 오래된 메시지 재생

- **파괴적 vs 비파괴적 소비**
    - **전통적 JMS/AMQP 큐**: 소비자가 메시지를 처리하고 **ACK** 하면 브로커가 메시지를 **삭제**(파괴적 소비) → 다시 읽어 재처리하기 어려움.
    - **로그 기반 브로커(예: Kafka)**: 메시지 소비는 **파일 읽기와 동일한 비파괴적 연산**. 로그는 **추가 전용(append-only)** 으로 남아 있어서 소비해도 원본이 보존됨.
- **오프셋이 유일한 소비자 상태**
    - 메시지 처리의 부수 효과는 소비자 **오프셋(offset) 이동**뿐.
    - 소비자 오프셋은 **소비자 측에서 관리·조작 가능** → 과거 시점으로 **rewind/seek** 하여 **재생(replay)** 할 수 있음.
- **재처리·백필(backfill)이 쉬움**
    - 예: “어제 처리한 끝 위치” 오프셋을 저장해두고 필요 시 그 지점으로 되감아 **어제분을 재처리** 가능.
    - **처리 코드가 바뀌어도** 오프셋만 조정해 **여러 번 재실행**해 새로운 파생 결과를 만들 수 있음.

## 3. 데이터베이스와 스트림

- **경계의 붕괴**
    - 로그 기반 브로커(Kafka 등)는 DB의 **append-only·복제** 아이디어를 차용해 고신뢰 전달을 구현.
    - 반대로 DB 쪽도 스트림 아이디어(CDC, 이벤트 소싱, 머티리얼라이즈드 뷰)를 수용해 **스트림↔DB 통합**이 가능.
- **로그의 의미 확장**
    - 로그는 단순 파일이 아니라 **데이터 전파·복제·복구·재처리의 매개**.
    - 불변성 덕분에 **되감기·재처리·백필**이 가능해 분석/버그 수정/재빌드에 유리.
- 데이터베이스의 기록은 본질적으로 이벤트 스트림이며, 스트림의 기반은 로그다. 이 ‘로그 관점’을 취하면 일관된 동기화와 재생이 가능한 데이터 시스템을 설계할 수 있다.

### 3-1. 시스템 동기화 유지하기

- 단일 기술로 **저장·질의·처리 요구를 모두 만족**시키기 어렵다. 실제 서비스는 OLTP DB(요청 처리), 캐시(응답 속도), 검색 색인(텍스트/랭킹), 데이터 웨어하우스(분석) 등 **여러 시스템을 병행**한다. 결과적으로 같은 비즈니스 데이터가 여러 곳에 복제되며 **동기화가 필수**가 된다.
- 동기화 기본 전략은 크게 두 갈래다.
    - **주기적 ETL(덤프→변환→적재)**: DB에서 데이터를 일정 주기로 추출해 DWH·검색·분석 시스템에 넣는다. 단순하고 원천 시스템에 미치는 영향이 적지만 **지연이 크고 최신성이 낮다**.
    - **듀얼 라이트(dual write)**: 애플리케이션이 **DB와 다른 시스템에 동시에 기록**한다. 지연은 낮지만 **원자성·순서 보장이 없어 레이스/부분실패로 영구 불일치**가 쉽게 생긴다.
        
        전형적 문제: 두 클라이언트가 같은 키를 A, B로 거의 동시에 갱신하면, **DB에는 B, 검색 색인에는 A** 같은 상태가 남을 수 있다. 애플리케이션이 두 시스템에 각각 쓰는 순간부터 **경쟁 조건**과 **순서 뒤집힘**을 피하기 어렵다.
        
        <img width="656" height="265" alt="image" src="https://github.com/user-attachments/assets/cc4e437d-f54e-4406-b4c2-f3d17f6f2619" />
        
    - 듀얼 라이트의 근본 해결로 자주 거론되는 것은 **분산 트랜잭션(XA/2PC)** 이다. 두 시스템을 한 트랜잭션으로 묶어 원자성을 보장하지만, **복잡·고비용·가용성 저하·장애 시 회복 어려움** 때문에 현대 분산/클라우드 환경에서는 거의 쓰지 않는다.
- 실용적인 대안은 **“하나의 사실원천(Single Source of Truth)을 정하고 나머지는 팔로워로 만든다”.**
    - 일반적으로 **DB를 리더**로 두고, DB의 **커밋 로그(LSN/변경 로그)** 를 **CDC(Change Data Capture)** 로 스트리밍한다.
    - 검색 색인·캐시·DWH는 **로그를 구독해 커밋 순서대로 적용**한다. 원천에 쓰기는 한 번만 하고, 파생 시스템은 **비동기 복제**로 따라간다.
    - 장점: **순서 보존·재처리 용이·정확히 한 번에 가까운 적용**(멱등/중복제거로 달성)·애플리케이션에서 이중쓰기 제거.
- 요약하면, 여러 저장소가 공존하는 현대 시스템에서 동기화의 핵심은 **“이중쓰기 금지, 한 곳을 사실원천으로, 변경 로그를 순서대로 전파”** 다. ETL은 단순하지만 느리고, 듀얼 라이트는 빠르지만 위험하다. 실무에서는 DB 커밋 로그를 기준으로 한 **CDC 기반 이벤트 스트리밍**이 일관성과 운영성을 함께 얻는 가장 현실적인 해법이다.

### 3-2. 변경 데이터 캡처

<img width="649" height="302" alt="image" src="https://github.com/user-attachments/assets/f34015ca-f288-4af2-b3ba-ce9edb36e1d5" />

- CDC, Change Data Capture: **DB에 커밋되는 모든 변경(INSERT/UPDATE/DELETE)을 트랜잭션 로그에서 읽어 ‘이벤트 스트림’으로 뽑아내는 방식**. 기록되는 즉시 외부로 내보낼 수 있어 거의 실시간.
- 핵심 특성
    - **순서 보존**: DB에 기록된 순서대로 변경이 스트리밍됨(X=A → X=B). 소비자(색인, 캐시, DWH)가 **같은 순서로 적용**해 최종 상태가 DB와 일치.
    - **일관성/복구 용이**: 로그 위치(LSN/오프셋)로 어디까지 처리했는지 추적 → 중단 후 **재시작·재처리**가 쉬움(중복 처리 시 멱등 처리로 안전).
    - **낮은 지연·부하**: 폴링 없이 로그를 **푸시/스트림**으로 소비 → 원천 DB에 쿼리 부하 적고 반영이 빠름.
    - **다중 소비자 확장**: 하나의 변경 스트림을 여러 시스템이 **구독**하여 각자 저장소에 **upsert/삭제**로 반영.
- 구현 포인트
    - DB가 **트랜잭션 로그 접근 API** 제공 필요(MySQL binlog, Postgres WAL, Oracle/SQL Server redo 등).
    - 변경·스키마 이벤트를 **외부 포맷(JSON 등)** 으로 내보내고, 소비자는 **오프셋 관리**와 멱등 적용을 구현.

### 3-3. 변경 데이터 캡처의 구현

- 동작 흐름: 트랜잭션 커밋 → DB의 변경 로그(WAL/binlog/oplog 등)에 기록 → CDC가 로그를 읽어 **이벤트 스트림** 생성 → 각 소비자가 **오프셋/LSN(Log Sequence Number)** 기준으로 순서대로 적용.
- **구현 옵션들: 트리거 기반, 로그 기반**
- **트리거 기반**
    - 각 테이블에 트리거를 달아 변경 시 “변경 테이블”에 기록.
    - 장점: DB가 로그 노출을 안 해도 구현 가능, 이해/디버깅이 직관적.
    - 단점: 스키마/트래픽 늘수록 **관리·성능 부담**(오버헤드, 장애 전파, 복잡성). 대규모 환경엔 취약.
- **로그(WAL/binlog/oplog) 기반**
    - DB의 **쓰기 로그를 파싱**해 변경 이벤트를 추출(“logical decoding” 계열).
    - 장점: **낮은 오버헤드**, 원자적 순서·트랜잭션 경계를 자연스럽게 보존, 대규모에 적합.
    - 단점: DB 내부 포맷/권한 의존, 과거엔 문서화가 부족했으나 최근 대부분 DB가 공식 지원(예: Postgres logical decoding, MySQL binlog, MongoDB oplog 등).
- **대표 도구/사례**
    - 오픈소스: **Debezium**(MySQL/Postgres/Kafka 연동), **Maxwell**(MySQL binlog→Kafka), **Bottled Water**(Postgres), **Mongoriver**(MongoDB), (과거) **LinkedIn Databus**, **Facebook Wormhole**.
    - 상용/매니지드: **Oracle GoldenGate** 등.

### 3-4. 초기 스냅숏

- **왜 스냅숏이 필요한가**: 모든 변경 로그를 끝없이 보관·재생하면 전체 상태를 재구축할 수 있지만, 저장공간/시간 비용이 커서 현실적이지 않다. 보통은 로그를 오래 못 보관하므로 **기준점(snap­shot) + 그 이후 변경 로그 재생** 패턴을 쓴다.
- **초기 스냅숏의 역할**: 새로 구축하는 검색 색인·데이터 웨어하우스 등 다운스트림 시스템은 먼저 DB의 현재 상태를 한 번에 떠온 ‘초기 스냅숏’으로 기저 데이터를 채우고, 이후에는 **변경 로그(CDC 스트림)** 를 이어 받아 증분 적용한다.
- **정합성의 핵심—오프셋 정렬**: 스냅숏에는 반드시 **해당 시점의 로그 위치(LSN/오프셋)** 가 함께 기록돼야 한다. 재시작/초기화 후엔 그 **같은 오프셋부터** 로그를 적용해야 빠짐·중복 없이 일관성이 맞는다.

### 3-5. 로그 컴팩션

- 로그 컴팩션은 append-only 로그를 **주기적으로 스캔해 같은 키의 기록을 합치고 가장 최신 값만 남기는** 백그라운드 정리(merge+compact)다.
- **삭제**는 값을 지우는 게 아니라 **툼스톤(tombstone, null/특별 마커)** 을 기록해 표시하고, 컴팩션 시 그 키의 과거 기록과 함께 완전히 제거한다.
- 컴팩션은 **CDC/로그 기반 브로커**와 궁합이 좋다. 같은 키의 이전 값을 덮어쓰거나(업데이트) 툼스톤을 기록(삭제)하면 된다.
- 새로운 소비자는 **토픽 오프셋 0부터 순차 스캔**만 해도 각 키의 **최신 상태를 재구성** 가능
- **Kafka는 로그 컴팩션을 지원**하므로, 단순 메시징을 넘어 **지속성 있는 저장소/복구 용도**로도 활용할 수 있다.

### 3-6. 변경 스트림용 API 지원

- 최근 데이터베이스가 **CDC(변경 데이터 캡처)** 를 자체 기능이나 리플리케이션 엔진을 통해 **실시간 변경 스트림 인터페이스**로 제공하기 시작했다.
- **Kafka Connect**는 다양한 DB·스토리지와 카프카를 연결하는 **범용 CDC/동기화 프레임워크**로, 변경 이벤트를 카프카 토픽으로 스트리밍해 검색 색인, 데이터 웨어하우스, 스트림 처리 파이프라인 등에 **실시간 공급**하도록 돕는다.
- 즉, DB 변경을 **순서 있는 이벤트 스트림**으로 외부에 내보내는 표준화가 확산되고 있으며, 이로써 애플리케이션은 ETL·동기화·색인·실시간 분석을 **느슨한 결합**으로 쉽게 구성할 수 있게 되었다.

### 3-7. 이벤트 소싱

- 이벤트 소싱이란? 시스템의 “현재 상태”를 직접 저장하지 않고, 도메인에서 일어난 **사건(Event)** 을 **append-only 불변 로그**로 시간순 기록한다. 실제 상태는 필요할 때 **이벤트 재생(replay)** 과 **프로젝션(읽기 모델 갱신)** 으로 재구성한다.
- **장점**: 전체 이력 보존(감사·추적), 과거 시점 **재현** 가능, 버그/장애 시 **replay** 로 복구·재계산 용이, 읽기 모델을 목적별로 **유연하게 추가**(검색/리포트 등), DDD와 궁합 좋음(업무 의미가 이벤트로 남음).
- **CDC와 대비**: **CDC**는 “DB가 원본, 변경을 사후 추출하여 스트리밍”. **이벤트 소싱**은 “**이벤트 로그가 원본**이고 DB 상태는 파생물”. 레거시 연동·단순 CRUD는 CDC가 적합, **감사/복원/다중 읽기뷰/도메인 이벤트가 중요한 시스템**은 이벤트 소싱이 적합.

### 3-8. 이벤트 로그에서 현재 상태 파생하기

- 로그(이벤트의 연속) 자체는 보기 힘들고 사용처가 제한적이므로, 사용자가 보려는 건 “현재 상태”다.
    
    → 이벤트 소싱 시스템은 **이벤트 로그 → 프로젝션(읽기 모델)** 로 **결정적으로 변환**하여 화면/쿼리용 상태를 만든다. 같은 로그라면 언제나 같은 상태가 나와야 한다.
    
- CDC vs 이벤트 소싱의 차이(재구성 관점)
    - **CDC/리두 로그 계열**: 레코드 수준 변경의 “가장 최신 버전”만 있으면 현재 상태를 복원 가능. 그래서 **로그 컴팩션**으로 과거 버전은 덜어내도 무방(최신값만 유지).
    - **이벤트 소싱**: 이벤트가 “도메인 행동”이어서 **부수 효과(상태 전이의 의미)** 를 담고 있음. 마지막 스냅샷 이후의 **전체 이벤트 이력**이 필요하며, **컴팩션으로 과거 이벤트를 임의 삭제할 수 없다.**

### 3-9. 명령과 이벤트

- **명령과 이벤트의 역할 구분**
    - **명령**: 사용자의 ‘의도/요청’. 아직 사실 아님. 비즈니스 규칙(무결성) 검사에 **실패할 수 있음**.
    - **이벤트**: **검증을 통과해 실제로 발생한 결과**(사실, Fact). 한 번 기록되면 **불변**으로 남음.
- **흐름**
    1. 클라이언트가 **명령**을 보냄
    2. 애플리케이션이 **동기적 검증**(재고/좌석 가능, 중복/권한 등)을 수행
    3. 통과하면 **도메인 이벤트**를 **영속 기록**하고 발행.
- **소비자 관점**
    - 이벤트는 이미 일어난 **사실**이므로 **소비자가 거절하지 않음**. 필요한 처리를 비동기로 수행(알림, 적립 등).
    - 따라서 **명령의 유효성 검증은 이벤트 발행 전에** 반드시 끝나야 함.
- **예시(좌석 예약)**
    - “좌석 X 예약해줘”는 **명령** → 검증 성공 시 “좌석 X가 사용자 Y에게 **예약되었다**”는 **이벤트**를 기록.
    - 이후 사용자가 취소해도 과거의 “예약되었다” 이벤트는 사실로 남고, 별도 **취소 이벤트**가 추가됨(정정이 아니라 **추가 기록**).
- **실무 패턴**
    - 트랜잭션 경계 안에서 **검증 + 상태변경 + 이벤트 기록**을 하나로 처리(직렬성 보장).
    - 대규모 시스템에선 **두 단계 이벤트**가 유용할 수 있음:
        - “예약 시도됨(요청)” → 비동기 확정 절차 → “예약 확정됨(확정)”. → 가예약 이벤트, 확정 이벤트
        - 이렇게 쪼개면 부하 완화·중복 방지·장애 복구가 쉬움.

### 3-10. 상태와 스트림 그리고 불변성

- **상태 vs 스트림**
    - 애플리케이션의 **현재 상태(state)** 는 시간에 따라 발생한 **이벤트 스트림(stream)** 을 “누적(적분)”한 결과다.
        
        → *state(now) = ∫ stream(t) dt*, *stream(t) = d state(t) / dt* 라는 비유로 설명.
        
    - 즉, 모든 **변경 로그(changelog)** 는 시간이 흐르며 바뀌는 상태를 나타내는 **이벤트들의 연속**이다.
- **불변 로그 = 진실의 원본**
    - 변경이 일어났다는 **사실(이벤트)은 불변**이다. 취소·정정 역시 **새 이벤트**로 기록한다.
    - **지속적으로 보관되는 변경 로그**가 있으면, 언제든 그 로그를 재생해 **현재 상태를 재구성(복구·재빌드)** 할 수 있다.
    - Pat Helland 관점: **로그가 진짜(원본)** 이고, 데이터베이스의 현재 상태는 **로그를 반영해 만든 캐시/머티리얼라이즈드 뷰**에 가깝다.
- **로그 컴팩션과의 연결**
    - 전체 히스토리를 항상 보관하지 않아도, **키별 최신값만 유지(컴팩션)** 하면 “현재 상태”를 빠르게 재구성 가능.
        
        → 과거 값은 제거되지만 **최신 스냅샷적 의미**는 유지된다.
        

### 3-11. 불변 이벤트의 장점

- **추가 전용 기록(append-only) = 회계 원장 모델**
    
    거래·행동을 삭제/덮어쓰기 하지 않고 ‘원장(ledger)’에 **사실(fact)** 로 누적 저장 → 히스토리 보존.
    
- **감사·추적·디버깅에 유리**
    
    잘못된 과거 데이터를 지우지 않고 **정정 이벤트**를 추가하므로, 누가·언제·무엇이 바뀌었는지 전 과정 재구성 가능.
    
- **복구 용이성**
    
    버그로 잘못 쓴 데이터도 원본 이벤트 스트림만 있으면 **재생(replay)** 하여 상태를 복원하기 쉬움.
    
- **분석 가치 확장**
    
    현재 스냅샷에는 남지 않는 **과정 정보(예: 장바구니에서 담았다가 뺀 항목)** 가 남아 마케팅·사용자 행동 분석에 활용 가능.
    

### 3-12. 동일한 이벤트 로그로 여러 가지 뷰 만들기

- **한 소스-멀티 뷰**: 하나의 **이벤트 로그**를 여러 소비자가 구독해 각자 **읽기 최적화 뷰(물질화 뷰)** 를 만든다. 예) Druid 같은 분석 스토어, 카프카 로그 커팅 스토어, **Kafka Connect Sink**로 다양한 DB/검색엔진으로 내보내기.
- **기존 시스템 무침범 확장**: 운영 DB를 건드리지 않고 **새 뷰를 추가**해 기능 확장·리포트·검색을 붙인다. 더 이상 안 쓰면 해당 뷰/시스템만 내려 **자원 회수**.
- **읽기/쓰기 분리(CQRS)**:
    - **쓰기 모델** = 이벤트 로그(쓰기 최적화).
    - **읽기 모델** = 사용 사례별 다수의 뷰(읽기 최적화, 인덱스·집계·캐시 등).
    - 변경 파이프라인이 **로그→뷰** 일관성을 유지.
- 요지는 “로그 하나로 다양한 읽기 전용 뷰를 독립적으로 만들고, 쓰기와 읽기를 분리(CQRS)해 확장성과 유연성을 얻는다”는 것.

### 3-13. 동시성 제어

- **문제 핵심**: 이벤트 로그는 **비동기 소비**가 기본이라, 방금 쓴 변경이 읽기 뷰(머터리얼라이즈드 뷰)에 **바로 반영되지 않을 수 있음**
- **해결책(강한 일관성 경로)**:
    
    로그 **append + 뷰 갱신을 하나의 트랜잭션**으로 묶어 **동기식** 처리.
    
    로그와 뷰가 **같은 저장소**면 쉬우나, 분리돼 있으면 **분산 트랜잭션/합의**가 필요해 비용·복잡도↑.
    

### 3-14. 불변성의 한계

- 불변 로그(append-only)를 영구히 쌓으면 저장·컴팩션·가비지 컬렉션 비용이 커지고 성능에도 부담이 생김. 쓰기·삭제가 잦은 워크로드에선 특히 문제.
- 규제/보안(계정 삭제, 개인정보 보호, 오기입 수정 등) 때문에 **실제 삭제**가 필요할 때가 있음. 단순 “추가만”으론 해결 안 됨.
- 그래서 시스템은 보존기간·TTL·컴팩션과 함께 **적출(excision)/shunning** 같은 “과거 기록 지우기/숨기기” 메커니즘을 준비해야 하고, **파생 뷰와 사본에도 삭제를 전파**할 방법이 필요함.
- 현실적으로 모든 사본·백업까지 완전 삭제는 어렵다 → 정책 기반 관리(보존 정책, 익명화, 삭제 워크플로)로 리스크를 줄이는 식으로 설계.

## 4. 스트림 처리

- 스트림으로 **무엇을 하느냐**는 크게 세 가지로 정리된다.
    - **동기화/적재**: 이벤트를 읽어 **DB·캐시·검색엔진 등 저장소**에 기록해서 다른 시스템과 상태를 맞춘다.
    - **즉시 알림**: 이벤트를 **사람/외부 시스템에 푸시**(알림, 웹훅 등). 이 경우 **사람이 최종 소비자**.
    - **파이프라인**: 하나 이상의 입력 스트림을 **연산자(operators)** 들로 처리해 **새로운 출력 스트림**을 만든다(필터, 집계, 윈도우, 조인 등). 여러 작업(job)을 이어 붙여 DAG로 운영.
- 스트림 처리 파이프라인은 10장(배치)의 데이터플로우와 매우 비슷하게 **분할·병렬화**한다. 차이는 **스트림은 끝이 없다**는 점.
- 끝이 없기 때문에 배치처럼 **전체 정렬·전역 그룹화/정렬-병합 조인** 같은 “전체 데이터가 있어야 가능한 연산”은 그대로 쓰기 어렵고, **시간 윈도우·증분 알고리즘**으로 바꿔야 한다.
- 장애 처리 관점도 다르다. 배치는 **실패한 태스크를 처음부터 재시작**하면 되지만, 스트림은 **오래 도는 작업**이라 그 방식이 비현실적이다. 그래서 **체크포인트, 재처리, (최소/정확히 한 번) 처리 보장** 같은 **내결함성 메커니즘**이 필수다.

### 4-1. 스트림 처리의 사용

- 원래 스트림 처리는 **모니터링/경보** 목적에서 오랫동안 쓰여 왔음.
- 대표 사례
    - **사기 감지**: 카드 사용 패턴이 예상과 다르면 즉시 차단/검토.
    - **거래 시스템**: 시장 **가격 변동**을 실시간 감지해 규칙 기반 자동 실행.
    - **제조 현장**: 설비 **상태 스트림**을 지속 관찰해 이상 조짐을 조기 탐지·조치.
    - **군/보안**: **침투·위협 징후**를 실시간 추적해 경보 발령.
- 이런 애플리케이션은 **시간 흐름 속 패턴 매칭·상관관계 분석**이 핵심이라 **복잡한 규칙/상태 관리**가 필요함.
- 최근엔 모니터링을 넘어, **다양한 비즈니스용 실시간 처리**(추천, 개인화, 실시간 ETL/동기화 등)로 활용 범위가 확대되는 추세.
- **키 포인트:** “시간에 따라 들어오는 사건을 즉시 해석하고 반응해야 할 때” 스트림 처리가 맞다.

### 4-2. 복잡한 이벤트 처리

- 복잡한 이벤트 처리(CEP, **Complex Event Processing)란?** 실시간 이벤트 스트림에서 **특정 패턴**(연속/누적/상관관계)을 규칙으로 표현해 찾아내고, 매칭되면 **알림·액션**을 발생시키는 방식. (문자열 정규식이 텍스트에서 패턴 찾듯이, CEP는 이벤트에서 패턴을 찾음)
- **동작 모델**
    - 사용자는 **선언형 질의/규칙**(중층 SQL 유사 DSL, 그래픽 룰 등)을 등록.
    - 엔진은 들어오는 스트림을 **윈도우(시간/개수)** 로 유지하며 **필터·집계·조인·순서 제약**을 평가.
    - 규칙이 충족되면 **복합 이벤트(Complex Event)** 를 생성해 하위 시스템으로 전달.
- **DB와의 차이**
    - DB: 데이터가 **영구 저장**되고 질의는 **일회성**으로 실행.
    - CEP: 질의(규칙)가 **오래 상주**하고, **흐르는 이벤트**를 계속 평가(“standing query”).
- **주요 표현/연산**
    - 시간/세션 **윈도우링**, 패턴(순서, 반복, 부재), **누적/슬라이딩 집계**, 스트림-스트림 **조인**, 이상 탐지 임계치.
    - 예: 5분 안에 동일 ID로 실패 로그인 3회 연속 → 경고
- **주요 활용**
    - 이상탐지/보안 경보, 거래 규칙 감시, IoT 센서 이상, 운영 모니터링, 광고/실시간 개인화 트리거.
- **대표 구현/플랫폼**
    - Esper, IBM InfoSphere Streams, TIBCO StreamBase, SQLstream 등
    - 분산 스트림 처리기(아파치 Samza 등)도 **SQL/패턴 확장**으로 CEP 기능 제공.

### 4-3. 스트림 분석

- **목표**: CEP가 “특정 패턴 탐지”에 초점이라면, 스트림 분석은 **연속적으로 들어오는 이벤트를 집계해 통계 지표**(빈도, 평균, 비정상 징후 등)를 뽑는 데 초점.
- **대표 지표**:
    - 이벤트 **빈도/발생률**(초‧분‧5분 단위 등)
    - **이동 평균(rolling average)** 같은 추세 지표
    - **전/평균 대비 이상치 탐지**(이번 주 vs 지난 주 등)
- **윈도우 기반 집계**: 고정 시간 간격의 **윈도우(window)** 로 묶어 집계(예: 최근 5분 평균). 초단위 잡음을 **평활(smoothing)** 해서 추세를 봄.
- **확률적 자료구조 활용**: 메모리/속도 절감을 위해 **블룸 필터**, **HyperLogLog**, 다양한 **백분위 추정** 알고리즘 사용 → **근사값**을 빠르게 제공(정확도보다 실시간성과 비용 효율을 중시).
- **도구/플랫폼**: Apache Storm, Spark Streaming, Flink, Concord, Samza, **Kafka Streams** 같은 오픈소스 프레임워크와 **Google Cloud Dataflow**, **Azure Stream Analytics** 같은 관리형 서비스가 대표적.
- **요점**: 스트림 분석은 **운영 모니터링/대시보드/알림** 용에 적합하며, **실시간 집계 + 근사 계산**을 결합해 대량 이벤트 흐름에서 **빠른 통찰**을 얻는 방식.

### 4-4. 구체화 뷰 유지하기

- **목적**: 원본 DB의 변경 스트림(CDC/이벤트 로그)을 읽어 **캐시·검색 색인·데이터 웨어하우스** 같은 **구체화 뷰(미리 계산된 읽기 모델)** 를 최신 상태로 계속 갱신한다.
- **동작 원리**: 기반 테이블에 변화가 생길 때마다 해당 변화만 **증분 적용**해서 뷰를 업데이트하므로, 쿼리 성능은 빠르고 쓰기 부하는 원본에 집중된다.
- **스트림 분석과 차이**: 분석은 보통 **유한한 시간 윈도우**만 보면 되지만, **구체화 뷰는 현재의 정확한 상태**를 만들기 위해 **해당 키의 모든(또는 마지막) 이벤트**가 필요하다. 그래서 **시작 시점부터의 누적 적용**(혹은 **로그 컴팩션**으로 키별 최신값만 보존)이 전제다.
- **도구/실무**: Samza, Kafka Streams 등은 **카프카의 로그 컴팩션**을 기반으로 로컬 스토어에 뷰를 유지·제공하는 패턴을 지원한다.
- **요점**: 변경 스트림을 **실시간으로 소비해 증분 갱신**하면, 원본 DB 변경과 **일관된 읽기 전용 뷰**를 효율적으로 유지할 수 있다.

### 4-5. 스트림 상에서 검색하기

- **정의:** 질의를 먼저 저장(지속 질의)해 두고, **흘러오는 이벤트가 질의에 매칭되면 즉시 알림**.
- **전통 검색과 차이:** 전통=문서 색인→질의 실행 / **스트림 검색=질의 색인→문서(이벤트) 도착 시 매칭**.
- **주요 용도:** 뉴스/소셜 키워드 모니터링, 부동산·채용 알림, Elasticsearch **Percolator**류.

### 4-6. 메시지 전달과 RPC

- **성격 차이:** RPC/액터는 **요청–응답 중심(동기·단기 연결)**, 메시지/이벤트는 **데이터 스트림 중심(비동기·지속성·다중 구독)**.
- **내결함성:** 이벤트 로그/브로커는 저장·재처리·구독을 **기본 제공**. 대부분의 RPC 프레임워크는 **전달 보장/재시도/멱등성**을 기본으로 보장하지 않음 → 별도 설계 필요.
- **파이프라인 배치:** 스트림 처리는 **비동기 파이프라인**에 질의/연산자를 고정해두고 데이터가 흘러들면 처리. RPC는 **호출 시점에 작업을 생성**해 결과를 돌려줌.
- **선택 기준:**
    - 짧은 트랜잭션·즉시 응답 필요 → **RPC/액터**
    - 이벤트 저장·재처리·다수 소비자 구독 → **메시지 브로커/로그 기반 스트림**
- **주의:** RPC로 스트림 흉내낼 수는 있지만 **장애 시 정확성(적어도/정확히 한 번)** 요구가 있으면 **브로커·오프셋·재처리 설계**를 붙여야 함.

### 4-6. 시간에 관한 추론

- 스트림 처리에선 “지난 5분 평균” 같은 **시간 윈도우**가 자주 쓰이는데, 이 “5분”이 무엇의 5분인지가 핵심이다.
    
    → **이벤트 시간(event time)**(사건이 실제로 발생한 시간) vs **처리 시간(processing time)**(시스템이 처리한 시간).
    
- 일괄 처리(batch)는 과거에 쌓인 데이터를 한 번에 읽어 처리하므로 시스템 시계를 신경 쓸 필요가 거의 없다.
    
    같은 입력이면 언제 돌려도 결과가 동일하며, 이벤트의 실제 타임스탬프만 보면 된다.
    
- 스트림 처리(streaming)는 보통 시스템의 처리 시간으로 윈도우를 자르기 쉬워서 많이 사용한다(단순·빠름).
    
    하지만 소비자가 느려지거나 지연이 커지면 “실제 5분”과 “처리 기준 5분”이 어긋나 **왜곡된 집계**가 생긴다.
    
- 핵심은 **내가 원하는 시간이 ‘사건의 시간’인지 ‘처리된 시간’인지 먼저 결정**하고, 그에 맞춰 윈도우·트리거(집계 확정 시점)·지연 처리 정책을 설계하는 것이다.

### 4-7. 이벤트 시간 대 처리 시간

- **개념**: 스트림에서 **이벤트 시간**은 사건이 실제로 발생한 시각, **처리 시간**은 시스템이 그 사건을 읽어 처리한 시각이다.
- **문제**: 장애·네트워크 지연·재처리(리플레이) 같은 일이 생기면 처리 시간이 뒤엉키면서 순서가 바뀌거나, 그래프에 스파이크처럼 **왜곡된 지표**가 나타난다.

<img width="603" height="347" alt="image" src="https://github.com/user-attachments/assets/49259d95-828e-4d04-ac9a-2ecf90a54e8e" />

### 4-8. 준비 여부 인식

- **문제의 본질**: 이벤트 *시간* 기준 윈도우를 쓰면, 해당 윈도우에 **모든 이벤트가 다 도착했는지 알기 어렵다**. 네트워크 지연·재전송 등으로 **늦게(늦도착/late) 들어오는 이벤트**가 존재함.
- **예시 상황**: “37분 윈도우”를 집계하는데 타임스탬프가 37분인 이벤트가 더 이상 안 올지 확신할 수 없음. 언제 “종료”를 선언하고 결과를 내보낼지 정책이 필요함.
- **처리 전략 두 가지**
    - **a) 늦게 도착 무시**: 마감 시점에 도착하지 못한 이벤트는 버림.
        - 장점: **지연 짧고 단순**.
        - 단점: **정확도 손실** 가능 → 늦게 도착 비율을 모니터링해 경보/튜닝.
    - **b) 수정 결과 재발행**: 먼저 결과를 내고, 이후 늦게 도착이 오면 **정정(업서트/리트랙션)** 을 다시 내보냄.
        - 장점: **정확도↑**.
        - 단점: 다운스트림도 **정정 처리**를 이해해야 하고 복잡도↑.
- **워터마크(watermark) 아이디어**: “이 시각보다 오래된 타임스탬프는 더 이상 안 온다”는 **특별 신호**로 윈도우 마감 시점을 추론. 분산 환경에선 **각 생산자별 최소 타임스탬프**를 추적하고 **합성 워터마크**로 결정.
- 요약하자면, 이벤트 시간 윈도우는 “언제 닫을지”가 핵심이며, **워터마크+허용 지연**으로 마감하고, **그 이후 늦게 도착을 버릴지(간단)** 또는 **정정으로 반영할지(정확)** 를 시스템 요구사항에 맞춰 선택하면 된다.

### 4-9. 어쨌든 어떤 시계를 사용할 것인가?

- 모바일 등 **여러 장치에서 비동기로 올라오는 이벤트**는 각자 시계가 달라서 시간 왜곡이 생김(오프라인 후 일괄 전송 포함).
- **장치 로컬 시계**는 “사용자 관점의 실제 발생 시각”을 담지만 **신뢰성(오차·오설정)** 문제가 큼. **서버 시계**는 안정적이지만 “사용자 상호작용의 실제 시점”을 설명하기엔 부족.
- 최선은 **타임스탬프 3개를 함께 기록**하는 것:
    1. 이벤트가 **발생한 시각(장치 시계)**,
    2. 이벤트를 **보낸 시각(장치 시계)**,
    3. 서버가 **받은 시각(서버 시계)**.
- (2)와 (3)의 차이를 활용해 **장치-서버 시계 오프셋을 추정**하고, 그 오프셋으로 (1)을 보정하면 **실제 발생 시간**을 더 정확히 재구성할 수 있음.
- 핵심: “어느 시계를 하나만 믿지 말고, **장치·전송·수신 시각을 함께 남겨** 오프셋을 추정·보정하라.”

### 4-10. 윈도우 유형

- 윈도우: “어느 구간의 이벤트를 모아 집계할지”를 시간 기준으로 자르는 방법. 평균/합계/카운트 같은 집계를 위해 범위를 정의한다.
- 텀블링(Tumbling) 윈도우
    - 크기 고정, 겹침 없음. 각 이벤트는 **정확히 한** 윈도우에만 포함.
    - 예: 1분 텀블링 → `10:03:00 ~ 10:03:59`, `10:04:00 ~ 10:04:59` …
    - 용도: 주기성 집계(분당/시간당 보고)
- 호핑(Hopping) 윈도우
    - **고정 길이 + 고정 간격(호프)**, 서로 겹침. 이벤트가 **여러** 윈도우에 들어갈 수 있음.
    - 예: 크기 5분, 호프 1분 → `10:03:00 ~ 10:07:59`, `10:04:00 ~ 10:08:59` …
    - 용도: 이동평균·부드러운 지표(슬라이딩 평균), 더 촘촘한 결과 필요할 때.
- 슬라이딩(Sliding) 윈도우
    - 경계가 고정되지 않음. **각 이벤트 시점 기준으로 직전 N분**을 창으로 삼아 연속 계산(사실상 호프=1 이벤트).
    - 예: 5분 슬라이딩이면 임의 시각마다 “직전 5분” 집계. → `10:03:39 ~ 10:08:12`
    - 용도: 실시간 알림/임계치 감시(“최근 5분 에러율”).
- 세션(Session) 윈도우
    - **비활동 간격(gap)** 기준으로 묶고, 일정 시간 새 이벤트가 없으면 창 종료. 같은 사용자가 짧은 시간 동안 발생시킨 모든 이벤트를 그룹화해서 세션 윈도우를 정의한다.
    - 크기 가변(사용자/디바이스별 트래픽에 따라 달라짐), 세션 병합 가능.
    - 용도: 사용자 행동 분석(방문/장바구니), 드문 이벤트를 “한 덩어리”로 보고 싶을 때.

### 4-11. 스트림 조인

- 스트림 처리도 일괄 처리처럼 조인이 필요할 수 있다. 차이는 데이터가 **끝없이 들어오고(무한)**, **늦게/순서 뒤바뀌어** 도착할 수 있어 조인이 더 어렵다는 점.

### 4-12. 스트림 스트림 조인 (윈도우 조인)

- **목적**: 서로 다른 이벤트 스트림(예: `검색` ↔ `클릭`)을 **같은 키**(세션 ID 등)로 **시간 윈도우 안에서 매칭**해 “이 검색 결과가 클릭되었는가?” 같은 질문에 답한다.
- **시간 불일치 대응**: 클릭이 검색보다 **늦게/먼저 도착**하거나 아예 안 올 수 있음 → **적절한 윈도우 크기**(예: 검색 후 5분)를 정해 그 안에서만 매칭 시도.
- **상태 유지**: 조인 처리기는 **윈도우 동안 키별 상태**(미매칭 검색/클릭 이벤트 목록)를 메모리/스토어에 보관한다. 새 이벤트가 오면 같은 키의 **반대편 이벤트가 있는지 조회**해 매칭.
- **출력 이벤트**:
    - 매칭되면: `검색-클릭 매칭` 이벤트를 즉시 방출(클릭 품질, 전환율 계산 등).
    - 윈도우 만료 시 미매칭 남으면: `클릭되지 않음`(또는 `검색 없음`)과 같은 **타임아웃 이벤트** 방출.
- **중요 파라미터**: 키 선정(세션/사용자/쿼리 등), **윈도우 종류/길이**, 지연·중복·역전 처리 정책(지각 도착 허용 범위, 중복 제거 키).
- **주의**: “검색 이벤트에 클릭 정보 추가”처럼 **단순 보강**이 아니라, **양쪽 스트림을 동시에 고려**하는 **조인 문제**다.

### 4-13. 스트림 테이블 조인 (스트림 강화)

- **개념**: 이벤트 스트림(활동 로그 등)에 **참조 테이블**(사용자 프로필, 가격표 등)의 속성을 **즉시 붙여서(enrich)** 내보내는 조인. 키는 보통 **사용자ID/상품ID** 같은 식별자.
- **접근 방식**
    - 처리 시점에 **DB 조회**해 붙인다 → 간단하지만 지연·부하 위험.
    - **로컬 캐시/로컬 복제본**(메모리/디스크)에 테이블을 유지하고 **키 조회**로 붙인다 → 스트림 처리에 적합.
- **신선도 보장**: 로컬 복제본은 **CDC/로그 컴팩션 토픽** 등으로 **지속 동기화**해야 함. 그래야 장기간 스트림 처리 중 테이블 변경도 반영.
- **시간 의미**: 스트림-스트림과 달리 **윈도우로 두 스트림을 맞출 필요는 없음**. 대신 “**그 이벤트가 발생한 시점의 테이블 값**”을 원하면 **버전/타임스탬프 기반 스냅샷**을 고려.
- **장점**: 낮은 지연으로 리치한 이벤트 생성(예: 사용자 등급, 지역, 캠페인정보 추가), 이후 **필터링/라우팅/집계**가 쉬워짐.
- **주의점**
    - DB 직접조회는 **폭주 시 병목**(큐 증가, 백프레셔) → 가급적 **로컬 상태 저장소** 사용.
    - 로컬 상태는 **복구·재시작 시 리빌드**가 필요하므로 **로그 기반 동기화**를 준비.
    - 테이블 변경 빈도가 높다면 **캐시 무효화 전략**(TTL/버전키)을 명확히.
- 요약 한 줄: **이벤트에 “정적(또는 느리게 변하는) 참조 데이터”를 키로 붙이는 조인**이며, 실전에서는 **로컬로 복제한 테이블을 CDC로 최신화**하며 처리하는 게 정석.

### 4-14. 테이블 테이블 조인 (구체화 뷰 유지)

- **문제**: 트위터 홈 타임라인처럼 “팔로우 집합 × 최신 트윗”을 매 조회마다 조인하면 비용이 너무 큼.
- **해결(구체화 뷰)**: 결과를 **캐시/물리 뷰**로 유지하고, **변경 이벤트**로 즉시 갱신.
    - 트윗 생성 → 해당 작성자를 **팔로우하는 모든 사용자 타임라인에 삽입**
    - 트윗 삭제 → 팔로워 타임라인에서 **제거**
    - 팔로우 시작 → 상대의 **최근 트윗을 대량 복사**해 타임라인에 채움
    - 언팔로우 → 상대 트윗을 **일괄 제거**
- **스트림 관점**: “트윗 스트림”과 “팔로우 관계 스트림/테이블”을 사용해 **물리 타임라인 뷰**를 실시간 유지.
- **SQL 관점**: `tweets JOIN follows` 결과를 **materialized view**처럼 보관하고, **두 테이블 중 하나가 바뀔 때마다** 그 결과를 갱신.
- **포인트**
    - 조회는 O(1) 단건 읽기로 빨라짐(쓰기 확장=팬아웃 비용 발생).
    - 대량 팔로우/언팔로우 시 **대량 삽입/삭제 작업**이 필요하므로 백프레셔/재시도 전략 필수.
    - 초기화·재빌드 시엔 CDC/로그 기반으로 뷰를 **재생성**하면 됨.

### 4-15. 조인의 시간 의존성

- **문제 정의**: 여러 스트림/테이블을 조인할 때 “어느 시점의 상태”와 조인할지가 핵심. 단일 파티션 내 순서는 보장되지만, **스트림 간/파티션 간 전역 순서는 보장되지 않아** 동일 입력이어도 재실행 결과가 달라질 수 있음(비결정성).
- **대표 사례**
    - 활동 이벤트 ↔ 사용자 프로필: 프로필 **수정 전** 상태로 조인할지, **수정 후** 상태로 조인할지 선택 필요.
    - 과거 판매 ↔ 세율: **거래 당시 세율**로 조인해야 재처리 시 정확함. 현재 세율로 조인하면 왜곡.
- **핵심 해법: SCD(천천히 변하는 차원) 버전 관리**
    - 차원 테이블(프로필·세율 등)에 **버전/유효기간(VALID_FROM/VALID_TO)** 을 둔다.
    - 이벤트에는 **event_time**(이벤트 시간)을 넣고, 조인 시 `event_time BETWEEN VALID_FROM AND VALID_TO` 로 **시점 일치 조인**을 수행.
    - 이 방식은 **모든 버전 보존**이 전제라서, 차원 테이블에 **로그 컴팩션(최신만 보존)** 을 쓰면 안 됨.

### 4-16. 내결함성

- **내결합성** 은 실패나 재시작이 있어도 **결과가 한 번 처리된 것과 동일하게 보장되는 성질**을 말한다.
- **일괄 처리(batch)** 는 입력이 불변이고 각 태스크 결과가 파일로 남기 때문에 재시작해도 동일 결과를 쉽게 얻을 수 있다.
- 일부 태스크가 실패하더라도 이전 결과와 **정확히 동일한 출력**을 얻을 수 있는 이유가 바로 이 내결합성 때문이다.
- 이를 **정확히 한 번 시맨틱(exactly-once semantics)** 또는 **결과적으로 한 번(effectively-once)** 이라 한다.
    
    → 실제로는 내부적으로 여러 번 처리되어도 **결과는 한 번 처리된 것과 동일하게** 보임.
    
- **스트림 처리(stream processing)** 에서도 이 문제가 중요하지만,
    - 스트림은 데이터가 **무한히 계속 흐르기 때문에**
    - “모든 태스크가 완료될 때까지 기다린다” 같은 일괄 처리 방식의 접근은 불가능하다.
- 따라서 스트림 처리에서는 **데이터 중복/재시도에도 불구하고 결과의 일관성을 유지**하도록 별도의 설계가 필요하다.

### 4-17. 마이크로 일괄 처리와 체크포인트

- **마이크로 일괄 처리**: 스트림을 1초 단위 등 작은 배치로 나눠 처리하는 방식. 실시간성은 높지만 조정 비용이 큼.
- **체크포인트**: 주기적으로 상태를 저장해 장애 시 복구 가능.
- **한계**: 외부 시스템으로 출력 직후 장애가 나면 **중복 처리(두 번 출력)** 발생 가능.
    
    → 즉, 마이크로 일괄 처리와 체크포인트만으로는 **정확히 한 번 처리(exactly-once)** 를 완전히 보장할 수 없음.
    

### 4-18. 원자적 커밋 재검토

- **정확히 한 번처럼 보이게 하려면 “모든 부수효과가 함께” 성공/실패**해야 함.
    
    (DB 쓰기, 외부 메시지 발송/푸시, 상태 변경, 입력 오프셋 커밋 이동 등)
    
- 이 효과는 **원자적으로 전부-or무(ALL-or-NOTHING)** 이어야 하며, 하나라도 어긋나면 **동기화가 깨져 “at-least/at-most once”** 상황이 됨.
- 전통적 해법은 **XA/2PC** 같은 분산 트랜잭션이지만, 비용·복잡도가 큼.
- 실무 대안: **프레임워크 내부에서 상태와 메시지를 함께 트랜잭셔널하게 관리**해 원자성에 준하는 보장을 제공.
    
    (예: Google Cloud Dataflow, VoltDB, 그리고 Kafka의 트랜잭셔널/Exactly-once 기능 계획/제공)
    
- 이런 내부 원자성은 **여러 입력을 한 트랜잭션으로 묶을 때 생기는 모호성**(순서/중복 등)을 **프레임워크가 흡수**해 결과 불일치를 줄이려는 접근.

### 4-19. 멱등성

- **목표**
  - 장애 후 재시작 시, 처리 결과가 두 번 나타나지 않도록 **안전하게 재처리**하는 것이 목적이다.
    → 실패한 작업의 중복 출력을 제거해야 한다.
    
- **핵심 개념**
    - **멱등 연산**은 여러 번 실행하더라도 결과가 한 번 실행한 것과 동일한 연산이다.
    - 예: 특정 키에 고정된 값을 설정(덮어쓰기)은 멱등함.
    - 반면, 카운터 증가처럼 누적되는 연산은 멱등하지 않음.
  
- **멱등성을 만드는 방법**
    - 완전한 멱등 연산이 아니더라도 **메타데이터(예: 메시지 오프셋)** 를 활용하면 멱등성처럼 동작하게 만들 수 있다.
    - Kafka의 경우, 메시지의 오프셋을 DB에 함께 기록하여 이미 처리된 메시지를 다시 수행하지 않게 함.

- **Storm의 Trident 예시**
    - Trident는 멱등성을 유지하기 위해 동일한 아이디 기반으로 상태를 관리한다.
    - 작업 재시작 시 **같은 순서로 동일한 메시지를 재생**해야 하며, 처리 결과는 **결정적(deterministic)** 이어야 한다 (즉, 같은 입력은 항상 같은 결과를 내야 함).
        
- **실제 의미**
    - 멱등성은 **정확히 한 번 시맨틱(exactly-once semantics)** 을 완벽히 보장하진 않지만,
    - 현실적으로는 **오버헤드가 적고 안정적인 대안**으로 사용된다.
    - 즉, 시스템이 죽거나 재시작되어도 “중복 없이 한 번 처리된 것처럼” 보이게 하는 실용적 방법이다.
    
### 4-20. 실패 후에 상태 재구축하기

- 스트림 처리는 집계나 조인처럼 **상태(state)** 가 필요한 연산이 많기 때문에 장애 발생 시 **상태 복구(recovery)** 가 필요하다.
- **원본 데이터 저장소 복원 방식**
    - 개별 메시지를 원래 데이터베이스에서 다시 질의해 상태를 복구.
    - 단점: 느리고 비효율적이라 실무에서는 거의 사용되지 않음.
- **로컬 상태 스냅숏 복제 방식**
    - 스트림 처리기가 로컬에 상태를 유지하고 주기적으로 **스냅숏(snapshot)** 저장.
    - 장애 발생 시 복제된 상태를 불러와 **데이터 손실 없이** 처리 재개 가능.
    - Flink: HDFS 등에 주기적 상태 저장.
    - Kafka Streams, Samza: **로그 컴팩션된 변경 스트림**을 통해 상태 복제.
- **입력 스트림 재생 방식**
    - 상태 복제가 필요 없을 정도로 단순한 경우, 입력 스트림을 다시 흘려보내 상태를 복원할 수 있음.
    - 짧은 윈도우 집계 등에 효과적.
- **트레이드오프 고려 사항**
    - 디스크 접근 시간, 네트워크 지연, 저장소 안정성 등 인프라 환경에 따라 최적의 복구 전략이 달라짐.

## 5. 정리

- **이벤트 스트림 처리의 개념**
    - 이벤트 스트림은 끝이 없는 데이터 흐름을 지속적으로 처리하는 방식이다.
    - 일괄 처리(batch)와 달리 실시간으로 이벤트를 받아 분석한다.
    - 메시지 브로커와 이벤트 로그는 스트림 처리에서 **파일 시스템과 유사한 역할**을 한다.
- **AMQP/JMS 스타일 메시지 브로커**
    - 개별 메시지를 소비자가 처리하고 확인 응답(ACK)을 보내면 메시지는 삭제된다.
    - 메시지의 순서보다는 처리 성공 여부가 중요하다.
    - 이미 처리된 메시지를 다시 읽을 필요가 없는 구조.
    - RPC(비동기 메시지 전송) 환경에 적합하다.
- **로그 기반 메시지 브로커**
    - 모든 메시지를 로그에 순서대로 기록하며, 각 소비자는 자신만의 오프셋을 관리한다.
    - Kafka와 같은 시스템이 이 방식에 해당한다.
    - 과거 메시지를 다시 읽어 재처리할 수 있으며, 장애 복구와 상태 추적에 유리하다.
    - **데이터베이스의 변경 로그(변경 데이터 캡처, CDC)** 와 유사한 접근 방식이다.
- **로그 기반 접근의 장점**
    - 입력 스트림을 소비해 **파생된 상태나 출력 스트림**을 만드는 데 적합.
    - 변경 로그를 통해 데이터베이스의 전체 상태를 스트림 상에서 유지 가능.
    - 여러 시스템 간 **동기화 및 재현(replay)** 에 용이하다.
    - 스트림을 재생하거나 조인하여 데이터의 최신 상태를 유지할 수 있다.
- **스트림 처리의 주요 조인 유형**
    - **스트림–스트림 조인**: 두 개의 이벤트 스트림을 시간 창(window) 내에서 매칭.
    - **스트림–테이블 조인**: 스트림 이벤트에 테이블(데이터베이스)의 현재 상태를 결합.
    - **테이블–테이블 조인**: 양쪽 모두 변경 로그를 기반으로 최신 상태를 유지하며 조인.
- **내결합성과 정확히 한 번 처리(Exactly-once)**
    - 스트림 처리는 장시간 실행되므로 일괄 처리처럼 단순히 출력물을 버릴 수 없다.
    - 따라서 **마이크로 배치, 체크포인팅, 트랜잭션, 멱등성(idempotence)** 등을 활용해 세밀한 단위로 복구 가능한 메커니즘을 사용한다.
