# 일괄 처리

높은 수준에서 데이터를 저장하고 처리하는 시스템은 크게 두 가지 범주로 그룹화할 수 있습니다.

* **기록 시스템 (Systems of record)**
  기록 시스템은 '진실의 원천(source of truth)'이라고도 하며, 데이터의 **권위 있는(authoritative) 버전**을 보유합니다. 사용자 입력과 같은 새로운 데이터가 들어오면 먼저 여기에 기록됩니다. 각 사실은 정확히 한 번만 표현됩니다(표현은 일반적으로 정규화됩니다). 다른 시스템과 기록 시스템 간에 불일치가 있는 경우, (정의에 따라) 기록 시스템의 값이 올바른 것입니다.

* **파생 데이터 시스템 (Derived data systems)**
  파생 시스템의 데이터는 다른 시스템의 기존 데이터를 가져와 어떤 방식이든 변환하거나 처리한 결과입니다. 파생 데이터를 잃어버리더라도 원본 소스에서 다시 만들 수 있습니다. 고전적인 예는 **캐시**입니다. 데이터가 캐시에 있으면 제공될 수 있지만, 캐시에 필요한 것이 없으면 기본 데이터베이스로 대체(fall back)할 수 있습니다. 비정규화된 값, 인덱스, 구체화된 뷰(materialized views)도 이 범주에 속합니다. 추천 시스템에서 예측 요약 데이터는 종종 사용 로그에서 파생됩니다.

기록 시스템과 파생 데이터 시스템의 구분은 도구가 아니라 애플리케이션에서 이를 어떻게 사용하느냐에 달려 있습니다.

이 책의 1부와 2부에서는 **요청(requests)** 과 **쿼리(queries)**, 그리고 그에 상응하는 **응답(responses)** 또는 **결과(results)** 에 대해 많이 이야기했습니다. 이러한 방식의 데이터 처리는 많은 최신 데이터 시스템에서 가정됩니다. 즉, 무언가를 요청하거나 명령을 보내면, 잠시 후 시스템이 답을 줍니다. 데이터베이스, 캐시, 검색 인덱스, 웹 서버 및 기타 많은 시스템이 이런 식으로 작동합니다.

세 가지 다른 유형의 시스템을 구분해 보겠습니다.

* **서비스 (온라인 시스템)**
  서비스는 클라이언트의 요청이나 지시가 도착하기를 기다립니다. 요청이 수신되면 서비스는 가능한 한 빨리 이를 처리하고 응답을 다시 보냅니다. **응답 시간**은 일반적으로 서비스의 주요 성능 척도이며, **가용성**도 종종 매우 중요합니다 (클라이언트가 서비스에 도달할 수 없으면 사용자는 아마도 오류 메시지를 보게 될 것입니다).

* **배치 처리 시스템 (오프라인 시스템)**
  배치 처리 시스템은 대량의 입력 데이터를 받아, 이를 처리하는 **잡(job)**을 실행하고, 일부 출력 데이터를 생성합니다. 잡은 종종 시간이 오래 걸리므로(몇 분에서 며칠까지), 일반적으로 잡이 끝나기를 기다리는 사용자는 없습니다. 대신, 배치 잡은 종종 주기적으로(예: 하루에 한 번) 실행되도록 예약됩니다. 배치 잡의 **주요 성능 척도는 일반적으로 처리량(throughput)**(특정 크기의 입력 데이터셋을 처리하는 데 걸리는 시간)입니다.

* **스트림 처리 시스템 (준실시간 시스템)**
  스트림 처리는 **온라인**과 **오프라인/배치** 처리의 중간쯤에 있습니다(그래서 때때로 **준실시간(near-real-time)** 또는 **니어라인(nearline)** 처리라고도 불립니다). 배치 처리 시스템처럼 스트림 프로세서는 입력을 소비하고 출력을 생성합니다(요청에 응답하기보다는). 그러나 스트림 잡은 이벤트가 발생한 직후에 작동하는 반면, 배치 잡은 고정된 입력 데이터 집합에 대해 작동합니다. 이러한 차이로 인해 스트림 처리 시스템은 동등한 배치 시스템보다 **더 낮은 지연 시간(latency)**을 가질 수 있습니다.

---

## Unix 도구를 사용한 배치 처리

다양한 도구가 로그 파일을 가져와 웹사이트 트래픽에 대한 멋진 보고서를 생성할 수 있습니다. (예: Unix 명령어 체인 대신 Ruby 스크립트 작성). 하지만 대용량 파일에서 분석을 실행할 때 분명해지는 실행 흐름에는 큰 차이가 있습니다.

이 간단한 Unix 명령어 체인은 메모리 부족 없이 대규모 데이터셋으로 쉽게 확장할 수 있습니다. 병목 현상은 아마도 디스크에서 입력 파일을 읽을 수 있는 속도일 것입니다.

Unix 설계 철학—자동화, 신속한 프로토타이핑, 점진적 반복, 실험에 친화적, 대규모 프로젝트를 관리 가능한 덩어리로 나누기—은 오늘날의 애자일(Agile) 및 데브옵스(DevOps) 운동과 놀랍도록 유사하게 들립니다.

그러나 Unix 도구의 가장 큰 한계는 단일 머신에서만 실행된다는 것입니다. 바로 이 지점에서 하둡(Hadoop)과 같은 도구가 등장합니다.

---

## MapReduce와 분산 파일 시스템

MapReduce는 Unix 도구와 다소 비슷하지만, 잠재적으로 수천 대의 머신에 분산되어 있습니다. 단일 MapReduce 잡은 단일 Unix 프로세스와 비슷합니다. 하나 이상의 입력을 받아 하나 이상의 출력을 생성합니다.

대부분의 Unix 도구와 마찬가지로, MapReduce 잡을 실행해도 일반적으로 입력이 수정되지 않으며 출력을 생성하는 것 외에는 다른 **부수 효과(side effects)** 가 없습니다. 출력 파일은 순차적으로 한 번만 기록됩니다.

Unix 도구가 `stdin`과 `stdout`을 입력과 출력으로 사용하는 반면, MapReduce 잡은 **분산 파일 시스템**의 파일을 읽고 씁니다. 하둡의 MapReduce 구현에서 이 파일 시스템은 HDFS(Hadoop Distributed File System)라고 불리며, 이는 구글 파일 시스템(GFS)을 오픈 소스로 재구현한 것입니다.

HDFS는 **공유 디스크(shared-disk)** 접근 방식과 대조적으로 **무공유(shared-nothing)** 원칙에 기반합니다. 공유 디스크 스토리지는 종종 맞춤형 하드웨어와 파이버 채널(Fibre Channel)과 같은 특수 네트워크 인프라를 사용하는 중앙 집중식 스토리지 어플라이언스에 의해 구현됩니다. 반면에, 무공유 접근 방식은 특별한 하드웨어가 필요 없으며, 단지 기존의 데이터센터 네트워크로 연결된 컴퓨터만 있으면 됩니다.

HDFS는 각 머신에서 실행되는 데몬 프로세스로 구성되어, 다른 노드가 해당 머신에 저장된 파일에 액세스할 수 있도록 네트워크 서비스를 노출합니다(데이터센터의 모든 범용 머신에는 일부 디스크가 연결되어 있다고 가정). **네임노드(NameNode)** 라는 중앙 서버는 어떤 파일 블록이 어떤 머신에 저장되어 있는지 추적합니다. 따라서 HDFS는 개념적으로 데몬을 실행하는 모든 머신의 디스크 공간을 사용할 수 있는 하나의 큰 파일 시스템을 생성합니다.

머신 및 디스크 장애를 견디기 위해 파일 블록은 여러 머신에 복제됩니다. 이 기술은 동일한 머신에 연결된 여러 디스크에 걸쳐 이중화(redundancy)를 제공하는 RAID와 유사합니다. 차이점은 분산 파일 시스템에서는 특수 하드웨어 없이 기존 데이터센터 네트워크를 통해 파일 액세스 및 복제가 수행된다는 것입니다.

### MapReduce 잡 실행

MapReduce는 HDFS와 같은 분산 파일 시스템에서 대용량 데이터셋을 처리하는 코드를 작성할 수 있게 해주는 프로그래밍 프레임워크입니다. MapReduce의 데이터 처리 패턴은 다음 예와 매우 유사합니다.

* 입력 파일 세트를 읽고 레코드로 분할합니다.
* **매퍼(mapper)** 함수를 호출하여 각 입력 레코드에서 키와 값을 추출합니다.
* 모든 키-값 쌍을 키 기준으로 정렬합니다.
* **리듀서(reducer)** 함수를 호출하여 정렬된 키-값 쌍을 순회합니다. 동일한 키가 여러 번 나타나면 정렬로 인해 목록에서 인접하게 되므로, 메모리에 많은 상태를 유지할 필요 없이 해당 값들을 쉽게 결합할 수 있습니다.

**매퍼**
매퍼는 모든 입력 레코드에 대해 한 번씩 호출되며, 그 역할은 입력 레코드에서 키와 값을 추출하는 것입니다. 각 입력에 대해 (없음을 포함하여) 임의의 수의 키-값 쌍을 생성할 수 있습니다. 한 입력 레코드에서 다음 입력 레코드로 상태를 유지하지 않으므로 각 레코드는 독립적으로 처리됩니다.

**리듀서**
MapReduce 프레임워크는 매퍼가 생성한 키-값 쌍을 가져와 동일한 키에 속하는 모든 값을 수집하고, 해당 값 컬렉션에 대한 이터레이터(iterator)와 함께 리듀서를 호출합니다. 리듀서는 (동일한 URL의 발생 횟수와 같은) 출력 레코드를 생성할 수 있습니다.

**MapReduce의 분산 실행**
Unix 명령어 파이프라인과의 주된 차이점은 MapReduce가 병렬 처리를 명시적으로 처리하는 코드를 작성하지 않고도 여러 머신에 걸쳐 계산을 **병렬화**할 수 있다는 것입니다. 매퍼와 리듀서는 한 번에 하나의 레코드에 대해서만 작동합니다. 입력이 어디서 오는지, 출력이 어디로 가는지 알 필요가 없으므로 프레임워크가 머신 간 데이터 이동의 복잡성을 처리할 수 있습니다.

<img width="723" height="520" alt="Image" src="https://github.com/user-attachments/assets/53219996-0986-489b-86d0-c38821b66508" />

그림 10-1은 하둡 MapReduce 잡의 데이터 흐름을 보여줍니다. 병렬화는 파티셔닝을 기반으로 합니다. 잡의 입력은 일반적으로 HDFS의 디렉터리이며, 입력 디렉터리 내의 각 파일 또는 파일 블록은 별도의 맵 태스크(m 1, m 2, m 3로 표시)로 처리될 수 있는 별도의 파티션으로 간주됩니다.

각 입력 파일은 일반적으로 수백 메가바이트 크기입니다. MapReduce 스케줄러(다이어그램에 표시되지 않음)는 해당 머신에 맵 태스크를 실행할 수 있는 충분한 예비 RAM 및 CPU 리소스가 있는 경우, 입력 파일의 복제본을 저장하는 머신 중 하나에서 각 매퍼를 실행하려고 시도합니다. 이 원칙을 **데이터 가까이에 계산을 배치(putting the computation near the data)**라고 합니다. 이는 네트워크를 통해 입력 파일을 복사하는 것을 절약하여 네트워크 부하를 줄이고 지역성(locality)을 높입니다.

리듀서별로 파티셔닝하고, 정렬하고, 매퍼에서 리듀서로 데이터 파티션을 복사하는 과정을 **셔플(shuffle)**이라고 합니다.

### MapReduce 워크플로우

단일 MapReduce 잡으로 해결할 수 있는 문제의 범위는 제한적입니다. 따라서 MapReduce 잡이 **워크플로우(workflows)**로 함께 연결되어 한 잡의 출력이 다음 잡의 입력이 되는 것이 매우 일반적입니다.

배치 잡의 출력은 잡이 성공적으로 완료되었을 때만 유효한 것으로 간주됩니다(MapReduce는 실패한 잡의 부분적인 출력을 폐기합니다). 따라서 워크플로우의 한 잡은 이전 잡(즉, 입력 디렉터리를 생성하는 잡)이 성공적으로 완료되었을 때만 시작할 수 있습니다.

이러한 스케줄러에는 대규모 배치 잡 컬렉션을 유지 관리할 때 유용한 관리 기능도 있습니다. 추천 시스템을 구축할 때 50~100개의 MapReduce 잡으로 구성된 워크플로우가 일반적이며, 대규모 조직에서는 여러 다른 팀이 서로의 출력을 읽는 다른 잡을 실행할 수 있습니다. 이러한 복잡한 데이터 흐름을 관리하는 데는 도구 지원이 중요합니다.

### 리듀스 측 조인(Reduce-Side Joins) 및 그룹화

많은 데이터셋에서 한 레코드가 다른 레코드와 연관 관계(관계형 모델의 외래 키, 문서 모델의 문서 참조, 그래프 모델의 엣지)를 갖는 것이 일반적입니다. **조인(join)**은 해당 연관 관계의 양쪽(참조를 보유한 레코드와 참조되는 레코드 모두)에 있는 레코드에 액세스해야 하는 코드가 있을 때마다 필요합니다. 비정규화는 조인의 필요성을 줄일 수 있지만 일반적으로 완전히 제거하지는 못합니다.

MapReduce 잡에 파일 세트가 입력으로 주어지면, 해당 파일의 전체 내용을 읽습니다. 데이터베이스는 이 작업을 **전체 테이블 스캔(full table scan)**이라고 부릅니다. 배치 처리의 맥락에서 조인에 대해 이야기할 때, 우리는 데이터셋 내의 일부 연관 관계의 모든 발생을 확인(resolving)하는 것을 의미합니다.

<img width="707" height="387" alt="Image" src="https://github.com/user-attachments/assets/2a0cced8-8fe6-44dc-88e5-84218cdda572" />

배치 프로세스에서 좋은 처리량을 얻으려면, 계산은 (가능한 한) 한 머신에 대해 로컬이어야 합니다. 처리하려는 모든 레코드에 대해 네트워크를 통해 임의 접근(random-access) 요청을 하는 것은 너무 느립니다. 더욱이, 원격 데이터베이스를 쿼리한다는 것은 원격 데이터베이스의 데이터가 변경될 수 있기 때문에 배치 잡이 **비결정적(nondeterministic)**이 된다는 것을 의미합니다.

따라서 더 나은 접근 방식은 사용자 데이터베이스의 사본을 (예: ETL 프로세스를 사용하여 데이터베이스 백업에서 추출하여) 가져와서 사용자 활동 이벤트 로그와 동일한 분산 파일 시스템에 두는 것입니다. 그러면 HDFS의 한 파일 세트에는 사용자 데이터베이스가, 다른 파일 세트에는 사용자 활동 레코드가 있게 되며, MapReduce를 사용하여 모든 관련 레코드를 같은 장소로 가져와 효율적으로 처리할 수 있습니다.

**정렬-병합 조인 (Sort-merge joins)**

<img width="709" height="397" alt="Image" src="https://github.com/user-attachments/assets/eb528069-6c60-4fbb-a26b-4738fcbf33d9" />

리듀서는 특정 사용자 ID에 대한 모든 레코드를 한 번에 처리하므로, 한 번에 하나의 사용자 레코드만 메모리에 유지하면 되며 네트워크를 통해 요청할 필요가 없습니다. 이 알고리즘은 **정렬-병합 조인**이라고 알려져 있습니다. 매퍼 출력이 키에 의해 정렬되고, 리듀서가 조인의 양쪽에서 정렬된 레코드 목록을 함께 병합하기 때문입니다.

**관련 데이터를 같은 장소로 가져오기**
MapReduce 프로그래밍 모델을 사용하면 계산의 물리적 네트워크 통신 측면(데이터를 올바른 머신으로 가져오기)이 애플리케이션 로직(데이터를 받은 후 처리하기)과 분리되었습니다. 이 분리는 데이터베이스의 일반적인 사용과 대조됩니다. 데이터베이스에서 데이터를 가져오라는 요청은 종종 애플리케이션 코드의 깊은 곳 어딘가에서 발생합니다. MapReduce가 모든 네트워크 통신을 처리하기 때문에, 다른 노드의 충돌과 같은 부분적인 장애에 대해 걱정할 필요가 없도록 애플리케이션 코드를 보호합니다. MapReduce는 애플리케이션 로직에 영향을 주지 않고 실패한 태스크를 투명하게 재시도합니다.

**skew 처리**
'동일한 키를 가진 모든 레코드를 같은 장소로 가져오는' 패턴은 단일 키와 관련된 데이터가 매우 많은 경우 무너집니다. 이렇게 비정상적으로 활동적인 데이터베이스 레코드를 **린치핀 객체(linchpin objects)** 또는 **핫 키(hot keys)** 라고 합니다. 유명인사(celebrity)와 관련된 모든 활동(예: 그들이 게시한 내용에 대한 답글)을 단일 리듀서에서 수집하면 심각한 **skew**(또는 **핫스팟(hot spots)**)가 발생할 수 있습니다. 즉, 한 리듀서가 다른 리듀서보다 훨씬 더 많은 레코드를 처리해야 합니다.

핫 키로 레코드를 그룹화하고 집계할 때, 그룹화를 두 단계로 수행할 수 있습니다. 첫 번째 MapReduce 단계는 레코드를 임의의 리듀서로 보내어, 각 리듀서가 핫 키에 대한 레코드의 하위 집합에 대해 그룹화를 수행하고 키당 더 압축된 집계 값을 출력하도록 합니다. 두 번째 MapReduce 잡은 모든 1단계 리듀서의 값을 키당 단일 값으로 결합합니다.

### 맵 측 조인 (Map-Side Joins)

**리듀스 측** 접근 방식은 입력 데이터에 대해 어떠한 가정도 할 필요가 없다는 장점이 있습니다. 데이터의 속성이나 구조가 어떻든 매퍼는 조인을 위해 데이터를 준비할 수 있습니다. 그러나 단점은 그 모든 정렬, 리듀서로의 복사, 리듀서 입력 병합이 상당히 비쌀 수 있다는 것입니다. 사용 가능한 메모리 버퍼에 따라, 데이터가 MapReduce 단계를 통과하면서 디스크에 여러 번 쓰일 수 있습니다.

반면에, 입력 데이터에 대해 특정 가정을 할 수 있다면 **맵 측 조인(map-side join)**을 사용하여 조인을 더 빠르게 만들 수 있습니다. 이 접근 방식은 리듀서와 정렬이 없는 축소된 MapReduce 잡을 사용합니다. 대신, 각 매퍼는 분산 파일 시스템에서 하나의 입력 파일 블록을 읽고 하나의 출력 파일을 파일 시스템에 씁니다. 그게 전부입니다.

**브로드캐스트 해시 조인 (Broadcast hash joins)**
맵 측 조인을 수행하는 가장 간단한 방법은 대규모 데이터셋이 소규모 데이터셋과 조인되는 경우에 적용됩니다. 특히, 소규모 데이터셋은 각 매퍼의 메모리에 전체를 로드할 수 있을 만큼 충분히 작아야 합니다.

이 간단하지만 효과적인 알고리즘을 **브로드캐스트 해시 조인**이라고 합니다. '브로드캐스트'라는 단어는 대규모 입력의 파티션에 대한 각 매퍼가 소규모 입력 전체를 읽는다는 사실을 반영하며(따라서 소규모 입력은 대규모 입력의 모든 파티션에 효과적으로 '브로드캐스트'됨), '해시'라는 단어는 해시 테이블을 사용함을 반영합니다.

**파티션된 해시 조인 (Partitioned hash joins)**
맵 측 조인의 입력이 동일한 방식으로 파티션된 경우, 해시 조인 접근 방식을 각 파티션에 독립적으로 적용할 수 있습니다.

파티셔닝이 올바르게 수행되면, 조인하려는 모든 레코드가 동일한 번호의 파티션에 위치하게 되므로, 각 매퍼가 각 입력 데이터셋에서 하나의 파티션만 읽는 것으로 충분합니다. 이는 각 매퍼가 더 적은 양의 데이터를 해시 테이블에 로드할 수 있다는 장점이 있습니다.

**맵 측 병합 조인 (Map-side merge joins)**
맵 측 조인의 또 다른 변형은 입력 데이터셋이 동일한 방식으로 파티션되었을 뿐만 아니라 동일한 키를 기준으로 **정렬**된 경우에 적용됩니다. 이 경우, 매퍼가 일반적으로 리듀서에 의해 수행되는 동일한 병합 작업을 수행할 수 있기 때문에 입력이 메모리에 들어갈 만큼 충분히 작은지는 중요하지 않습니다. 즉, 두 입력 파일을 키 오름차순으로 증분적으로 읽고 동일한 키를 가진 레코드를 매칭합니다.

---

## 배치 워크플로우의 출력

데이터베이스 쿼리의 경우, 트랜잭션 처리(OLTP) 목적과 분석(OLAP) 목적을 구분했습니다.

배치 처리는 어디에 해당할까요? 트랜잭션 처리도 아니고 분석도 아닙니다. 배치 프로세스가 일반적으로 입력 데이터셋의 많은 부분을 스캔한다는 점에서 분석에 더 가깝습니다. 그러나 MapReduce 잡의 워크플로우는 분석 목적으로 사용되는 SQL 쿼리와 동일하지 않습니다. 배치 프로세스의 출력은 종종 보고서가 아니라 다른 종류의 구조입니다.

**검색 인덱스 구축**
구글이 MapReduce를 사용한 원래 목적은 검색 엔진을 위한 인덱스를 구축하는 것이었으며, 이는 5~10개의 MapReduce 잡 워크플로우로 구현되었습니다.

고정된 문서 집합에 대해 전체 텍스트 검색을 수행해야 하는 경우, 배치 프로세스는 인덱스를 구축하는 매우 효과적인 방법입니다. 매퍼가 필요에 따라 문서 집합을 파티셔닝하고, 각 리듀서가 해당 파티션에 대한 인덱스를 구축하며, 인덱스 파일은 분산 파일 시스템에 기록됩니다. 이러한 **문서-파티션 인덱스**를 구축하는 것은 병렬화가 매우 잘 됩니다.

**배치 프로세스 출력으로서의 키-값 저장소**
배치 처리를 위한 또 다른 일반적인 용도는 분류기(예: 스팸 필터, 이상 징후 감지, 이미지 인식) 및 추천 시스템(예: 알 수도 있는 사람, 관심 있을 만한 제품 또는 관련 검색어)과 같은 머신 러닝 시스템을 구축하는 것입니다.

이러한 배치 잡의 출력은 종종 어떤 종류의 데이터베이스입니다. 예를 들어, 사용자 ID로 쿼리하여 해당 사용자에 대한 추천 친구를 얻을 수 있는 데이터베이스 또는 제품 ID로 쿼리하여 관련 제품 목록을 얻을 수 있는 데이터베이스입니다.

**배치 프로세스 출력의 철학**
이 장의 앞부분에서 논의한 Unix 철학은 데이터 흐름에 대해 매우 명시적임으로써 실험을 장려합니다. 프로그램은 입력을 읽고 출력을 씁니다. 그 과정에서 입력은 변경되지 않은 상태로 유지되고, 이전 출력은 새 출력으로 완전히 대체되며, 다른 부수 효과는 없습니다. 이는 시스템 상태를 엉망으로 만들지 않고 원하는 만큼 명령을 재실행하고, 수정하거나 디버깅할 수 있음을 의미합니다.

MapReduce 잡의 출력 처리도 동일한 철학을 따릅니다. 입력을 **불변(immutable)**으로 처리하고 부수 효과(예: 외부 데이터베이스에 쓰기)를 피함으로써, 배치 잡은 우수한 성능을 달성할 뿐만 아니라 유지 관리도 훨씬 쉬워집니다.

---

## 하둡과 분산 데이터베이스 비교

앞서 보았듯이, 하둡은 다소 분산된 버전의 Unix와 같습니다. HDFS는 파일 시스템이고 MapReduce는 Unix 프로세스의 기발한 구현입니다(맵 단계와 리듀스 단계 사이에 항상 정렬 유틸리티를 실행하는).

MapReduce 논문이 발표되었을 때, 어떤 의미에서는 전혀 새로운 것이 아니었습니다. 지난 몇 섹션에서 논의한 모든 처리 및 병렬 조인 알고리즘은 이미 10여 년 전에 소위 **대규모 병렬 처리(MPP)** 데이터베이스에 구현되어 있었습니다.

가장 큰 차이점은 MPP 데이터베이스는 머신 클러스터에서 분석 SQL 쿼리의 병렬 실행에 중점을 두는 반면, MapReduce와 분산 파일 시스템의 조합은 임의의 프로그램을 실행할 수 있는 범용 운영 체제와 훨씬 더 유사한 것을 제공한다는 것입니다.

**스토리지의 다양성**
데이터베이스는 데이터를 특정 모델(예: 관계형 또는 문서)에 따라 구조화해야 하는 반면, 분산 파일 시스템의 파일은 단지 바이트 시퀀스일 뿐이며, 어떤 데이터 모델과 인코딩을 사용해서도 작성할 수 있습니다. 이는 데이터베이스 레코드의 컬렉션일 수도 있지만, 텍스트, 이미지, 비디오, 센서 판독값, 희소 행렬, 피처 벡터, 게놈 서열 또는 다른 어떤 종류의 데이터일 수도 있습니다.

직설적으로 말하면, 하둡은 HDFS에 데이터를 무차별적으로 덤핑하고 나중에 처리 방법을 파악할 수 있는 가능성을 열었습니다. 대조적으로, MPP 데이터베이스는 일반적으로 데이터를 데이터베이스의 독점적인 스토리지 형식으로 가져오기 전에 데이터 및 쿼리 패턴에 대한 신중한 사전 모델링이 필요합니다.

아이디어는 **데이터 웨어하우스**와 유사합니다. 대규모 조직의 여러 부분에서 데이터를 한 곳으로 가져오는 것만으로도 가치가 있습니다. 이전에는 이질적이었던 데이터셋 간의 조인이 가능해지기 때문입니다. MPP 데이터베이스에서 요구하는 신중한 스키마 설계는 이러한 중앙 집중식 데이터 수집을 느리게 합니다. 원시 형태로 데이터를 수집하고 나중에 스키마 설계를 걱정하면 데이터 수집 속도를 높일 수 있습니다(이 개념은 때때로 '데이터 레이크' 또는 '엔터프라이즈 데이터 허브'라고도 함).

무차별적인 데이터 덤핑은 데이터 해석의 부담을 이동시킵니다. 데이터셋 생산자에게 표준화된 형식으로 데이터를 가져오도록 강요하는 대신, 데이터 해석은 소비자의 문제가 됩니다(**읽기 시 스키마(schema-on-read)** 접근 방식). 이는 생산자와 소비자가 서로 다른 우선순위를 가진 다른 팀일 경우 이점이 될 수 있습니다. 이상적인 데이터 모델이 하나만 있는 것이 아니라, 데이터에 대한 여러 관점이 다른 목적에 적합할 수 있습니다. 데이터를 원시 형태로 덤핑하면 여러 가지 변환이 가능합니다. 이 접근 방식은 **스시 원칙(sushi principle)** 이라고 불립니다.

따라서 하둡은 종종 ETL 프로세스를 구현하는 데 사용되었습니다.

**처리 모델의 다양성**
MPP 데이터베이스는 디스크의 스토리지 레이아웃, 쿼리 계획, 스케줄링 및 실행을 처리하는 모놀리식(monolithic)이며 긴밀하게 통합된 소프트웨어입니다. 이러한 모든 구성 요소가 데이터베이스의 특정 요구에 맞게 조정되고 최적화될 수 있으므로, 시스템 전체가 설계된 유형의 쿼리에서 매우 우수한 성능을 달성할 수 있습니다. 또한 SQL 쿼리 언어는 코드 작성 없이도 표현력이 풍부한 쿼리와 우아한 시맨틱을 허용하므로 비즈니스 분석가가 사용하는 그래픽 도구(예: 태블로)에서 접근할 수 있습니다.

SQL과 MapReduce라는 두 가지 처리 모델만으로는 충분하지 않았습니다. 훨씬 더 다양한 모델이 필요했습니다! 그리고 하둡 플랫폼의 개방성 덕분에, 모놀리식 MPP 데이터베이스의 한계 내에서는 불가능했을 다양한 접근 방식을 구현하는 것이 가능했습니다.

하둡 생태계에는 HBase와 같은 임의 접근 OLTP 데이터베이스와 Impala와 같은 MPP 스타일 분석 데이터베이스가 모두 포함됩니다. HBase와 Impala 모두 MapReduce를 사용하지 않지만, 둘 다 스토리지를 위해 HDFS를 사용합니다. 이들은 데이터를 액세스하고 처리하는 매우 다른 접근 방식이지만, 그럼에도 불구하고 동일한 시스템에서 공존하고 통합될 수 있습니다.

**빈번한 장애에 대한 설계**
MapReduce를 MPP 데이터베이스와 비교할 때, 두 가지 설계 접근 방식의 차이점이 더 두드러집니다. 바로 장애 처리와 메모리 및 디스크 사용입니다. 배치 프로세스는 실패하더라도 사용자에게 즉시 영향을 미치지 않고 언제든지 다시 실행할 수 있으므로 온라인 시스템보다 장애에 덜 민감합니다.

쿼리 실행 중에 노드가 충돌하면 대부분의 MPP 데이터베이스는 전체 쿼리를 중단하고, 사용자가 쿼리를 다시 제출하도록 하거나 자동으로 다시 실행합니다. 쿼리는 일반적으로 최대 몇 초 또는 몇 분 동안 실행되므로, 재시도 비용이 그리 크지 않기 때문에 이러한 오류 처리 방식은 수용 가능합니다. MPP 데이터베이스는 또한 디스크에서 읽는 비용을 피하기 위해 가능한 한 많은 데이터를 메모리(예: 해시 조인 사용)에 유지하는 것을 선호합니다.

반면에 MapReduce는 맵 또는 리듀스 태스크의 실패가 전체 잡에 영향을 미치지 않도록 개별 태스크 단위로 작업을 재시도함으로써 장애를 견딜 수 있습니다. 또한 부분적으로는 내결함성을 위해, 부분적으로는 어차피 데이터셋이 메모리에 담기에는 너무 클 것이라는 가정 하에 데이터를 디스크에 매우 적극적으로 씁니다. MapReduce 접근 방식은 더 큰 잡에 더 적합합니다.

---

## MapReduce를 넘어서

MapReduce가 2000년대 후반에 매우 인기를 얻고 많은 과대광고를 받았지만, 이는 분산 시스템을 위한 많은 가능한 프로그래밍 모델 중 하나일 뿐입니다. 데이터의 양, 데이터의 구조, 수행되는 처리 유형에 따라 계산을 표현하는 데 다른 도구가 더 적절할 수 있습니다.

### 중간 상태의 구체화(Materialization)

분산 파일 시스템의 잘 알려진 위치에 데이터를 게시하면, 잡이 누가 입력을 생성하는지 또는 누가 출력을 소비하는지 알 필요가 없도록 **느슨한 결합(loose coupling)**이 가능합니다.

그러나 많은 경우에, 한 잡의 출력이 동일한 팀에서 유지 관리하는 다른 하나의 잡의 입력으로만 사용된다는 것을 알고 있습니다. 이 경우 분산 파일 시스템의 파일은 단순히 **중간 상태(intermediate state)**입니다. 즉, 한 잡에서 다음 잡으로 데이터를 전달하는 수단입니다.

이 중간 상태를 파일에 쓰는 과정을 **구체화(materialization)**라고 합니다.

대조적으로, 이 장의 시작 부분에 있는 로그 분석 예제는 Unix 파이프를 사용하여 한 명령의 출력을 다른 명령의 입력에 연결했습니다. 파이프는 중간 상태를 완전히 구체화하지 않고, 대신 작은 인메모리 버퍼만 사용하여 출력을 입력으로 증분적으로 **스트리밍**합니다.

**데이터플로우 엔진 (Dataflow engines)**
MapReduce의 이러한 문제를 해결하기 위해, 분산 배치 계산을 위한 몇 가지 새로운 실행 엔진(Spark, Tez, Flink)이 개발되었습니다. 설계 방식에는 다양한 차이점이 있지만, 전체 워크플로우를 독립적인 하위 잡으로 나누는 대신 **하나의 잡으로 처리**한다는 공통점이 있습니다. 여러 처리 단계를 통한 데이터의 흐름을 명시적으로 모델링하기 때문에 이러한 시스템을 **데이터플로우 엔진**이라고 합니다.

MapReduce와 달리, 이러한 함수는 교대로 맵과 리듀스의 엄격한 역할을 맡을 필요 없이, 더 유연한 방식으로 조립될 수 있습니다. 우리는 이러한 함수를 **연산자(operators)**라고 부르며, 데이터플로우 엔진은 한 연산자의 출력을 다른 연산자의 입력에 연결하기 위한 여러 가지 옵션을 제공합니다.

**내결함성 (Fault tolerance)**
중간 상태를 분산 파일 시스템에 완전히 구체화하는 것의 장점은 **내구성(durable)**이 있다는 것이며, 이는 MapReduce에서 내결함성을 상당히 쉽게 만듭니다. 태스크가 실패하면, 다른 머신에서 다시 시작하여 파일 시스템에서 동일한 입력을 다시 읽으면 됩니다.

Spark, Flink, Tez는 중간 상태를 HDFS에 쓰는 것을 피하므로, 장애를 견디기 위해 다른 접근 방식을 취합니다. 머신이 실패하고 해당 머신의 중간 상태가 손실되면, 여전히 사용 가능한 다른 데이터(가능하다면 이전 중간 단계, 그렇지 않다면 일반적으로 HDFS에 있는 원본 입력 데이터)로부터 **재계산**됩니다.

**구체화에 대한 논의**
Unix 비유로 돌아가면, MapReduce는 각 명령의 출력을 임시 파일에 쓰는 것과 같은 반면, 데이터플로우 엔진은 Unix 파이프와 훨씬 더 유사해 보입니다. 특히 Flink는 파이프라인 실행 아이디어를 중심으로 구축되었습니다. 즉, 연산자의 출력을 다른 연산자로 증분적으로 전달하고, 처리를 시작하기 위해 입력이 완료될 때까지 기다리지 않습니다.

잡이 완료되면, 사용자가 찾아서 사용할 수 있도록 출력이 내구성 있는 곳(아마도 다시 분산 파일 시스템)에 기록되어야 합니다. 따라서 데이터플로우 엔진을 사용할 때도, HDFS의 구체화된 데이터셋은 여전히 일반적으로 잡의 입력과 최종 출력입니다. MapReduce와 마찬가지로 입력은 불변이며 출력은 완전히 대체됩니다. MapReduce에 비해 개선된 점은 모든 중간 상태를 파일 시스템에 쓰는 수고를 덜 수 있다는 것입니다.

---

## 그래프와 반복 처리

배치 처리 컨텍스트에서 그래프를 살펴보는 것은 흥미롭습니다. 여기서 목표는 전체 그래프에 대해 일종의 오프라인 처리나 분석을 수행하는 것입니다. 이러한 필요성은 종종 추천 엔진과 같은 머신 러닝 애플리케이션이나 랭킹 시스템에서 발생합니다. 예를 들어, 가장 유명한 그래프 분석 알고리즘 중 하나는 **페이지랭크(PageRank)**로, 다른 웹 페이지가 어떤 웹 페이지를 링크하는지에 따라 웹 페이지의 인기도를 추정하려고 시도합니다. 이는 웹 검색 엔진이 결과를 제시하는 순서를 결정하는 공식의 일부로 사용됩니다.

많은 그래프 알고리즘은 한 번에 하나의 엣지를 순회하고, 일부 정보를 전파하기 위해 하나의 정점을 인접한 정점과 조인하며, 어떤 조건이 충족될 때까지 반복하는 방식으로 표현됩니다.

그래프를 분산 파일 시스템(정점과 엣지의 목록을 포함하는 파일)에 저장하는 것은 가능하지만, '완료될 때까지 반복'한다는 이 아이디어는 MapReduce가 데이터를 한 번만 통과하기 때문에 일반 MapReduce로는 표현할 수 없습니다. 따라서 이런 종류의 알고리즘은 종종 **반복적(iterative)** 스타일로 구현됩니다.

이 접근 방식은 작동하지만, MapReduce로 구현하는 것은 종종 매우 비효율적입니다. MapReduce는 알고리즘의 반복적인 특성을 고려하지 않기 때문입니다. 그래프의 작은 부분만 마지막 반복에 비해 변경되었더라도 항상 전체 입력 데이터셋을 읽고 완전히 새로운 출력 데이터셋을 생성할 것입니다.

---

## 상위 수준 API 및 언어

이전에 논의했듯이, Hive, Pig, Cascading, Crunch와 같은 상위 수준 언어와 API는 MapReduce 잡을 직접 프로그래밍하는 것이 상당히 힘들기 때문에 인기를 얻었습니다. Tez가 등장함에 따라, 이러한 상위 수준 언어는 잡 코드를 다시 재작성할 필요 없이 새로운 데이터플로우 실행 엔진으로 이동할 수 있다는 추가적인 이점을 갖게 되었습니다. Spark와 Flink도 자체적인 상위 수준 데이터플로우 API를 포함하고 있습니다.

이러한 데이터플로우 API는 일반적으로 계산을 표현하기 위해 관계형 스타일의 빌딩 블록을 사용합니다. 즉, 일부 필드의 값을 기준으로 데이터셋 조인하기, 키로 튜플 그룹화하기, 일부 조건으로 필터링하기, 카운팅, 합산 또는 기타 함수로 튜플 집계하기 등입니다. 내부적으로 이러한 작업은 이 장의 앞부분에서 논의한 다양한 조인 및 그룹화 알고리즘을 사용하여 구현됩니다.

**선언적 쿼리 언어로의 이동**
조인 알고리즘의 선택은 배치 잡의 성능에 큰 차이를 만들 수 있으며, 이 장에서 논의한 모든 다양한 조인 알고리즘을 이해하고 기억할 필요가 없다는 것은 좋은 일입니다. 이는 조인이 **선언적** 방식으로 지정되면 가능합니다. 애플리케이션은 단순히 어떤 조인이 필요한지 명시하고, **쿼리 옵티마이저**가 이를 가장 잘 실행할 수 있는 방법을 결정합니다.

그러나 다른 방식에서 MapReduce와 그 데이터플로우 후속 엔진들은 SQL의 완전한 선언적 쿼리 모델과는 매우 다릅니다. MapReduce는 **함수 콜백** 아이디어를 중심으로 구축되었습니다. 각 레코드 또는 레코드 그룹에 대해 사용자 정의 함수(매퍼 또는 리듀서)가 호출되며, 해당 함수는 무엇을 출력할지 결정하기 위해 임의의 코드를 자유롭게 호출할 수 있습니다. 이 접근 방식은 파싱, 자연어 분석, 이미지 분석, 수치 또는 통계 알고리즘 실행과 같은 작업을 수행하기 위해 기존 라이브러리의 대규모 생태계를 활용할 수 있다는 장점이 있습니다.

임의의 코드를 쉽게 실행할 수 있는 자유는 오랫동안 MapReduce 계열의 배치 처리 시스템을 MPP 데이터베이스와 구별하는 특징이었습니다.

상위 수준 API에 선언적 측면을 통합하고 실행 중에 이를 활용할 수 있는 쿼리 옵티마이저를 가짐으로써, 배치 처리 프레임워크는 MPP 데이터베이스와 더 유사해지기 시작합니다(그리고 비슷한 성능을 달성할 수 있습니다). 동시에, 임의의 코드를 실행하고 임의의 형식으로 데이터를 읽을 수 있는 확장성을 가짐으로써 유연성 이점을 유지합니다.

배치 처리 엔진은 점점 더 광범위한 도메인의 알고리즘을 분산 실행하는 데 사용되고 있습니다. 배치 처리 시스템이 내장 기능과 상위 수준의 선언적 연산자를 얻고, MPP 데이터베이스가 더 프로그래밍 가능하고 유연해짐에 따라, 둘은 점점 더 비슷해 보이기 시작합니다. 결국, 이들은 모두 데이터를 저장하고 처리하기 위한 시스템일 뿐입니다.





